1
00:00:00,000 --> 00:00:03,080
I saw a headline this week that actually made me laugh out loud.

2
00:00:03,080 --> 00:00:08,680
It was a serious tech article, but the premise, it just sounded like a setup for a bad sci-fi

3
00:00:08,680 --> 00:00:09,680
movie.

4
00:00:09,680 --> 00:00:10,680
Oh, yeah.

5
00:00:10,680 --> 00:00:13,480
The author claimed they sat down and interviewed a piece of software.

6
00:00:13,480 --> 00:00:15,360
It does sound a little ridiculous when you put it like that.

7
00:00:15,360 --> 00:00:16,360
Right.

8
00:00:16,360 --> 00:00:21,680
And I don't mean they interviewed the CEO or, you know, the lead engineer, not at all.

9
00:00:21,680 --> 00:00:27,440
They claimed to have interviewed the code itself, specifically this new architecture behind

10
00:00:27,440 --> 00:00:28,440
the Claude code.

11
00:00:28,440 --> 00:00:33,640
They asked it, point blank, how do you actually work without messing everything up?

12
00:00:33,640 --> 00:00:38,440
It's a bold approach, but the article we're digging into today by Zoltan Erdos is actually

13
00:00:38,440 --> 00:00:39,440
quite brilliant.

14
00:00:39,440 --> 00:00:45,320
It's called how Claude code engineers context and interview with Opus 4.6.

15
00:00:45,320 --> 00:00:47,400
So we're taking this interview seriously then.

16
00:00:47,400 --> 00:00:51,800
We have to because what Erdos uncovered isn't just about one tool.

17
00:00:51,800 --> 00:00:56,880
It's really a masterclass in what he calls context engineering.

18
00:00:56,880 --> 00:00:57,880
Okay.

19
00:00:57,880 --> 00:01:02,560
It sounds impressive, but it also sounds a little like a buzz word invented to sell consulting

20
00:01:02,560 --> 00:01:03,560
hours.

21
00:01:03,560 --> 00:01:06,160
I get the skepticism I do.

22
00:01:06,160 --> 00:01:10,200
We spent the last couple of years hearing all about prompt engineering, you know, finding

23
00:01:10,200 --> 00:01:13,360
the magic spell to cast on the AI to get the right answer.

24
00:01:13,360 --> 00:01:14,360
The magic words.

25
00:01:14,360 --> 00:01:15,360
Exactly.

26
00:01:15,360 --> 00:01:16,360
Context engineering is different.

27
00:01:16,360 --> 00:01:17,360
It's not about the words.

28
00:01:17,360 --> 00:01:19,160
It's about the architecture of the information itself.

29
00:01:19,160 --> 00:01:20,160
Yeah.

30
00:01:20,160 --> 00:01:24,120
It's about how you manage the AI's, well, it's limited working memory.

31
00:01:24,120 --> 00:01:28,000
So it doesn't get confused or distracted or just plain stupid.

32
00:01:28,000 --> 00:01:32,680
That's the part I really want to get into because I think most people, myself included,

33
00:01:32,680 --> 00:01:34,680
assume computers don't get distracted.

34
00:01:34,680 --> 00:01:35,680
They're machines.

35
00:01:35,680 --> 00:01:37,480
They process data.

36
00:01:37,480 --> 00:01:42,000
But this article suggests that if you feed an AI too much information, you're actually

37
00:01:42,000 --> 00:01:43,000
hurting it.

38
00:01:43,000 --> 00:01:44,000
You're not just hurting it.

39
00:01:44,000 --> 00:01:46,600
You're effectively lowering its IQ.

40
00:01:46,600 --> 00:01:48,040
And that's what we need to explore today.

41
00:01:48,040 --> 00:01:53,200
The physics of why that happens and how this new plan mode feature in Claude code actually

42
00:01:53,200 --> 00:01:54,200
fixes it.

43
00:01:54,200 --> 00:01:55,200
Okay.

44
00:01:55,200 --> 00:01:56,200
Let's start right there.

45
00:01:56,200 --> 00:01:57,200
Plan mode.

46
00:01:57,200 --> 00:02:00,360
The article describes this moment where you hit shift plus tab in the interface and the

47
00:02:00,360 --> 00:02:02,360
whole AI just changes.

48
00:02:02,360 --> 00:02:06,120
It stops trying to write code and starts to what contemplate.

49
00:02:06,120 --> 00:02:07,640
Ideally, yeah.

50
00:02:07,640 --> 00:02:11,680
To the user, it feels like the AI has shifted gears into a brainstorming mode.

51
00:02:11,680 --> 00:02:12,680
Right.

52
00:02:12,680 --> 00:02:16,920
But the interview reveals the mechanic is much simpler and honestly kind of funny.

53
00:02:16,920 --> 00:02:19,720
The system isn't switching to a bigger smarter brain.

54
00:02:19,720 --> 00:02:21,760
It's just slapping restriction on the model.

55
00:02:21,760 --> 00:02:22,760
A restriction.

56
00:02:22,760 --> 00:02:24,240
It's a constraint.

57
00:02:24,240 --> 00:02:28,920
When you enter a plan mode, the system injects a hidden prompt that says essentially, you

58
00:02:28,920 --> 00:02:33,200
are strictly prohibited from creating, modifying or deleting files.

59
00:02:33,200 --> 00:02:35,600
So it's putting the AI in handcuffs.

60
00:02:35,600 --> 00:02:36,600
Contextual handcuffs.

61
00:02:36,600 --> 00:02:37,600
Yeah.

62
00:02:37,600 --> 00:02:38,600
You could say that.

63
00:02:38,600 --> 00:02:39,880
But think about why.

64
00:02:39,880 --> 00:02:44,560
If the AI can write code, it will try to solve your problem immediately.

65
00:02:44,560 --> 00:02:46,160
It'll just guess jumps the gun.

66
00:02:46,160 --> 00:02:47,480
It does.

67
00:02:47,480 --> 00:02:52,440
By removing its ability to act, the system forces the model to explore first.

68
00:02:52,440 --> 00:02:56,880
It creates this space where the only thing the AI is allowed to do is read, search and

69
00:02:56,880 --> 00:02:57,880
strategize.

70
00:02:57,880 --> 00:02:59,120
That seems counterintuitive.

71
00:02:59,120 --> 00:03:01,760
You make the AI smarter by taking away its tools.

72
00:03:01,760 --> 00:03:03,640
You make it more deliberate.

73
00:03:03,640 --> 00:03:08,360
And the output of this mode, the plan, it generates, that isn't just a to-do list for you.

74
00:03:08,360 --> 00:03:10,120
This is the key insight.

75
00:03:10,120 --> 00:03:15,640
The plan is a context engineered prompt for the next version of the AI that will actually

76
00:03:15,640 --> 00:03:16,640
do the coding.

77
00:03:16,640 --> 00:03:17,640
Explain that.

78
00:03:17,640 --> 00:03:19,480
Why does the next version need a prompt?

79
00:03:19,480 --> 00:03:21,480
Can't it just remember what it just thought about?

80
00:03:22,120 --> 00:03:25,560
And this is the fundamental limitation of large language models that I think people

81
00:03:25,560 --> 00:03:26,560
forget all the time.

82
00:03:26,560 --> 00:03:27,560
Which is?

83
00:03:27,560 --> 00:03:29,080
They have zero long-term memory.

84
00:03:29,080 --> 00:03:30,080
None.

85
00:03:30,080 --> 00:03:31,080
None.

86
00:03:31,080 --> 00:03:35,120
Every time you send a message, the AI is, for all intents and purposes, waking up for

87
00:03:35,120 --> 00:03:36,120
the first time.

88
00:03:36,120 --> 00:03:37,680
It has total amnesia.

89
00:03:37,680 --> 00:03:40,680
It only knows what is currently in its context window.

90
00:03:40,680 --> 00:03:43,520
It's like that movie, a memento.

91
00:03:43,520 --> 00:03:45,080
That is the perfect analogy.

92
00:03:45,080 --> 00:03:49,560
The AI is the guy with amnesia and the plan is the tattoo he writes on his arm so he

93
00:03:49,560 --> 00:03:51,760
knows what to do when he wakes up five minutes later.

94
00:03:51,760 --> 00:03:56,440
So the explorer AI looks around, figures out the problem, writes this plan.

95
00:03:56,440 --> 00:04:00,480
Then the system wipes its memory, wakes up the coder AI, and just says, read this tattoo

96
00:04:00,480 --> 00:04:01,840
and get to work.

97
00:04:01,840 --> 00:04:03,520
That is exactly what happens.

98
00:04:03,520 --> 00:04:06,960
And that tattoo the plan, it has to be incredibly specific.

99
00:04:06,960 --> 00:04:13,000
If the explorer found a bug in, say, ASRC component center dot T-S-X, it needs to write that exact

100
00:04:13,000 --> 00:04:14,000
file path in the plan.

101
00:04:14,000 --> 00:04:15,000
Right.

102
00:04:15,000 --> 00:04:16,000
If it doesn't?

103
00:04:16,000 --> 00:04:17,760
The coder AI won't know that file even exists.

104
00:04:17,760 --> 00:04:20,040
This brings us to the big question I had reading this.

105
00:04:20,040 --> 00:04:22,800
The explorer goes out and reads files to build this plan.

106
00:04:22,800 --> 00:04:25,080
Why do we even need this two step process?

107
00:04:25,080 --> 00:04:30,600
Why can't I just upload my entire code base, all 10,000 files, and just say, fix the bug?

108
00:04:30,600 --> 00:04:31,600
You could try.

109
00:04:31,600 --> 00:04:32,600
Yeah.

110
00:04:32,600 --> 00:04:34,800
But this is where we run into what the article calls the physics of attention.

111
00:04:34,800 --> 00:04:35,800
Physics.

112
00:04:35,800 --> 00:04:37,120
It's a hardware reality.

113
00:04:37,120 --> 00:04:42,560
In these models, every single piece of text you feed it, the code, the comments, even

114
00:04:42,560 --> 00:04:45,920
the white space is broken down into these things called tokens.

115
00:04:45,920 --> 00:04:46,920
Right.

116
00:04:46,920 --> 00:04:50,280
The model uses a mechanism called self attention to understand them.

117
00:04:50,280 --> 00:04:56,160
Basically, every token has to look at every other token to figure out how they all relate.

118
00:04:56,160 --> 00:04:57,160
OK.

119
00:04:57,160 --> 00:04:59,400
So if I have a thousand words, every word is checking in with every other word.

120
00:04:59,400 --> 00:05:00,400
Yes.

121
00:05:00,400 --> 00:05:03,400
And here is the kicker that most people just don't realize.

122
00:05:03,400 --> 00:05:05,480
Attention is a zero sum game.

123
00:05:05,480 --> 00:05:10,640
Imagine the AI's attention is a pie, a single fixed size pie.

124
00:05:10,640 --> 00:05:14,480
That pie represents 100% of the focus the model can give.

125
00:05:14,480 --> 00:05:15,480
I like pie.

126
00:05:15,480 --> 00:05:20,640
So if you feed the AI, say, a thousand tokens of relevant code, the stuff that actually

127
00:05:20,640 --> 00:05:25,720
has the bug that code, get the nice big slice of the attention pie, the AI can see it

128
00:05:25,720 --> 00:05:26,720
clearly.

129
00:05:26,720 --> 00:05:27,720
Makes sense.

130
00:05:27,720 --> 00:05:31,920
But if you then dump in 10,000 tokens of irrelevant documentation or random PDF files

131
00:05:31,920 --> 00:05:36,000
or old chat while slicing the pie thinner, you're decimating the pie.

132
00:05:36,000 --> 00:05:39,760
You're forcing the model to spread that same amount of attention over 10 times as much

133
00:05:39,760 --> 00:05:40,760
data.

134
00:05:40,760 --> 00:05:44,120
The signal of the actual bug gets completely diluted.

135
00:05:44,120 --> 00:05:46,520
So it's not just that the extra info is clutter.

136
00:05:46,520 --> 00:05:51,160
It literally makes the AI stupider regarding the specific task I gave it.

137
00:05:51,160 --> 00:05:53,200
Stupider or at least much easier.

138
00:05:53,200 --> 00:05:56,120
The article describes irrelevant context as toxic.

139
00:05:56,120 --> 00:05:58,160
It actively degrades the signal.

140
00:05:58,160 --> 00:05:59,600
That's why you get hallucinations.

141
00:05:59,600 --> 00:06:03,760
The AI is trying to find a pattern in a sea of noise and just start seeing things that

142
00:06:03,760 --> 00:06:04,760
aren't there.

143
00:06:04,760 --> 00:06:05,760
That actually explains a lot.

144
00:06:05,760 --> 00:06:09,960
I've had times where I've paced in a huge error log and the AI just seems to lose

145
00:06:09,960 --> 00:06:10,960
its mind.

146
00:06:10,960 --> 00:06:12,960
It's a grounding in noise.

147
00:06:12,960 --> 00:06:15,400
And there's another layer to this physics problem.

148
00:06:15,400 --> 00:06:17,000
It's not just about attention.

149
00:06:17,000 --> 00:06:19,360
It's about the cost of reading versus writing.

150
00:06:19,360 --> 00:06:24,600
The article mentioned a price difference, something like $5 for input versus $25 for

151
00:06:24,600 --> 00:06:25,600
output.

152
00:06:25,600 --> 00:06:26,600
Roughly that, yeah.

153
00:06:26,600 --> 00:06:32,400
Input tokens, what the AI reads are cheap, output tokens, what it writes are expensive.

154
00:06:32,400 --> 00:06:33,400
Why?

155
00:06:33,400 --> 00:06:36,960
I would have thought reading a whole book takes more energy than writing a summary.

156
00:06:36,960 --> 00:06:40,240
For a human, yes, for a GPU, it's the opposite.

157
00:06:40,240 --> 00:06:44,320
When you feed a prompt to an AI, it processes all those words in parallel.

158
00:06:44,320 --> 00:06:48,760
It's one giant mathematical operation, a matrix multiplication that happens all at once.

159
00:06:48,760 --> 00:06:50,160
It's incredibly efficient.

160
00:06:50,160 --> 00:06:51,160
Pre-fill.

161
00:06:51,160 --> 00:06:52,360
That's the pre-fill phase.

162
00:06:52,360 --> 00:06:56,400
But generating the answer, the decode phase, that is sequential.

163
00:06:56,400 --> 00:06:58,920
The AI has to guess the first word.

164
00:06:58,920 --> 00:07:01,640
Then it feeds that word back into itself to guess the second word.

165
00:07:01,640 --> 00:07:04,560
Then the third, one by one.

166
00:07:04,560 --> 00:07:08,800
So reading is like glancing at a photo, but writing is like painting a copy of that photo

167
00:07:08,800 --> 00:07:10,840
by hand, pixel by pixel.

168
00:07:10,840 --> 00:07:11,840
Perfect analogy.

169
00:07:11,840 --> 00:07:16,840
And because writing ties up the hardware for so much longer, it costs way more.

170
00:07:16,840 --> 00:07:20,880
This economic reality drives how these systems are architected.

171
00:07:20,880 --> 00:07:23,600
It's why they want to use something called crompt caching.

172
00:07:23,600 --> 00:07:27,800
I saw that term 92% prefix reuse rate.

173
00:07:27,800 --> 00:07:29,360
That sounds like recycling.

174
00:07:29,360 --> 00:07:30,880
It basically is.

175
00:07:30,880 --> 00:07:34,880
If the beginning of your conversation, the system prompt, the list of tools, the project

176
00:07:34,880 --> 00:07:35,880
rules.

177
00:07:35,880 --> 00:07:40,240
If that stuff doesn't change, the AI doesn't have to reread it every single time.

178
00:07:40,240 --> 00:07:42,080
It just remembers the math it did last time.

179
00:07:42,080 --> 00:07:46,880
So it's like, if I ask you a question, you don't have to reread my biography to know who

180
00:07:46,880 --> 00:07:47,880
I am.

181
00:07:47,880 --> 00:07:48,880
You have that cast.

182
00:07:48,880 --> 00:07:53,440
And because that cast memory is cheap, about 10% of the cost of reading new stuff, want

183
00:07:53,440 --> 00:07:55,840
to keep the start of your prompt exactly the same.

184
00:07:55,840 --> 00:07:57,920
You never put dynamic stuff at the top.

185
00:07:57,920 --> 00:07:59,680
You append the new questions to the bottom.

186
00:07:59,680 --> 00:08:03,320
So the structure is always static rules at the top, which are cheap and cash, and then

187
00:08:03,320 --> 00:08:05,320
the new messy data at the bottom.

188
00:08:05,320 --> 00:08:06,320
Exactly.

189
00:08:06,320 --> 00:08:09,320
But there is a danger in just appending new data forever.

190
00:08:09,320 --> 00:08:11,640
We circle right back to the attention pie.

191
00:08:11,640 --> 00:08:14,520
If the chat gets too long, you run into context rot.

192
00:08:14,520 --> 00:08:15,520
That sounds gross.

193
00:08:15,520 --> 00:08:17,520
Is that like bit rot?

194
00:08:17,520 --> 00:08:18,520
Worse.

195
00:08:18,520 --> 00:08:21,320
It's when the AI starts believing its own lies.

196
00:08:21,320 --> 00:08:26,640
Let's say early in the chat, the AI hallucinates a function called get user data v2.

197
00:08:26,640 --> 00:08:27,640
It doesn't exist.

198
00:08:27,640 --> 00:08:28,640
The AI just made it up.

199
00:08:28,640 --> 00:08:29,640
Okay.

200
00:08:29,640 --> 00:08:32,360
But now that hallucination is in the chat history.

201
00:08:32,480 --> 00:08:34,480
It's in the context window.

202
00:08:34,480 --> 00:08:36,680
Ten minutes later, you ask for a fix.

203
00:08:36,680 --> 00:08:41,760
The AI looks back, sees get user data v2 in the history and thinks, ah, that must be

204
00:08:41,760 --> 00:08:42,760
real.

205
00:08:42,760 --> 00:08:43,760
It's right there in the transcript.

206
00:08:43,760 --> 00:08:44,760
It gaslights itself.

207
00:08:44,760 --> 00:08:46,200
It absolutely does.

208
00:08:46,200 --> 00:08:49,680
It treats its own past output as absolute truth.

209
00:08:49,680 --> 00:08:51,360
So how does cloud code handle this?

210
00:08:51,360 --> 00:08:54,960
If I'm working on a huge project, the chat is going to get massive.

211
00:08:54,960 --> 00:08:56,920
Am I just doomed to rot?

212
00:08:56,920 --> 00:08:59,360
They use a strategy called compaction.

213
00:08:59,360 --> 00:09:02,000
When the conversation history hits about 83% of the limit.

214
00:09:02,000 --> 00:09:05,280
The system essentially takes a chainsaw to the middle of the conversation.

215
00:09:05,280 --> 00:09:06,280
It just deletes it.

216
00:09:06,280 --> 00:09:07,280
It summarizes it.

217
00:09:07,280 --> 00:09:10,960
It throws away the blow by blow details of your previous errors and just keeps a high

218
00:09:10,960 --> 00:09:12,280
level summary.

219
00:09:12,280 --> 00:09:18,560
But, and this is the clever part, it re-injects the critical stuff, the tattoo, the plan file,

220
00:09:18,560 --> 00:09:23,720
and the clawed.ed.md file, which holds your project rules are protected.

221
00:09:23,720 --> 00:09:26,240
They get pasted back in after the purge.

222
00:09:26,240 --> 00:09:30,920
So the AI loses the noise of the argument you just had, but keeps the instructions on

223
00:09:30,920 --> 00:09:32,200
what to do next.

224
00:09:32,200 --> 00:09:36,360
This feels like a very, very elaborate way to manage a very specific weakness.

225
00:09:36,360 --> 00:09:37,360
It is.

226
00:09:37,360 --> 00:09:41,840
And it highlights the biggest difference between human intelligence and AI intelligence.

227
00:09:41,840 --> 00:09:42,840
Which is?

228
00:09:42,840 --> 00:09:43,840
Metacognition.

229
00:09:43,840 --> 00:09:47,840
Knowing what you don't know, if you ask me about quantum physics, I can stop and say,

230
00:09:47,840 --> 00:09:49,840
I have no idea, let me go to the library.

231
00:09:49,840 --> 00:09:51,280
But the AI just keeps talking.

232
00:09:51,280 --> 00:09:53,640
The AI has no doubt mechanism.

233
00:09:53,640 --> 00:09:58,800
If a fact is in its context window, even it's a hallucination, it treats it as reality.

234
00:09:58,800 --> 00:10:00,880
It cannot step back and audit its own memory.

235
00:10:00,920 --> 00:10:02,720
Which is why we still need humans.

236
00:10:02,720 --> 00:10:03,880
For now, yeah.

237
00:10:03,880 --> 00:10:07,880
The article argues, our role is shifting from code writer to assumption auditor.

238
00:10:07,880 --> 00:10:11,280
We need to look at that plan and say, wait, why do you think that file exists?

239
00:10:11,280 --> 00:10:13,360
We have to check the AI's work before it commits.

240
00:10:13,360 --> 00:10:15,200
Okay, I want to pivot here.

241
00:10:15,200 --> 00:10:19,040
We've geeked out on the internal mechanics, the handcuffs, the attention pie, the gas

242
00:10:19,040 --> 00:10:20,040
lighting.

243
00:10:20,040 --> 00:10:22,120
But a lot of our listeners aren't using clawed code.

244
00:10:22,120 --> 00:10:25,800
They're using chat GPT or standard clawed or Gemini.

245
00:10:25,800 --> 00:10:29,800
The interface might be different, but the brain works the same way.

246
00:10:29,800 --> 00:10:31,000
So how do we apply this?

247
00:10:31,000 --> 00:10:36,320
If I'm sitting in front of a standard chatbot, how do I context engineer my way to a better

248
00:10:36,320 --> 00:10:37,640
answer?

249
00:10:37,640 --> 00:10:39,960
You have to act like the architecture we just described.

250
00:10:39,960 --> 00:10:41,760
You have to be the sub agent.

251
00:10:41,760 --> 00:10:42,760
Be the intern.

252
00:10:42,760 --> 00:10:43,760
Be the intern.

253
00:10:43,760 --> 00:10:47,800
So tactic number one, explore before you prompt.

254
00:10:47,800 --> 00:10:50,760
Most people just paste an error message and screen help.

255
00:10:50,760 --> 00:10:51,760
Guilty.

256
00:10:51,760 --> 00:10:52,760
Don't do that.

257
00:10:52,760 --> 00:10:55,120
You are polluting the context with panic.

258
00:10:55,120 --> 00:10:58,280
Instead, find the files that are likely involved.

259
00:10:58,280 --> 00:11:00,400
That's the content of those files first.

260
00:11:00,400 --> 00:11:03,560
Prime the context window with the signal, then ask the question.

261
00:11:03,560 --> 00:11:08,880
So do the manual labor of gathering the ingredients before you ask the AI to cook?

262
00:11:08,880 --> 00:11:09,880
Precisely.

263
00:11:09,880 --> 00:11:11,400
Tactic number two, reference.

264
00:11:11,400 --> 00:11:12,560
Don't describe.

265
00:11:12,560 --> 00:11:14,320
Humans are terrible at describing code.

266
00:11:14,320 --> 00:11:17,520
We say things like, you know, that pattern we use in the other file with the hooks.

267
00:11:17,520 --> 00:11:19,360
And the AI has no idea what you mean.

268
00:11:19,360 --> 00:11:20,360
It's guessing.

269
00:11:20,360 --> 00:11:25,320
Instead, just say, see scutlesauce.ts, follow that pattern.

270
00:11:25,320 --> 00:11:27,120
The AI is an incredible mimic.

271
00:11:27,120 --> 00:11:32,000
If you give it a concrete example in the context, it will copy the spacing, the naming conventions,

272
00:11:32,000 --> 00:11:33,960
the error handling, everything.

273
00:11:33,960 --> 00:11:36,040
It's much higher bandwidth than your English description.

274
00:11:36,040 --> 00:11:37,040
Okay.

275
00:11:37,040 --> 00:11:38,520
What about the context rot issue?

276
00:11:38,520 --> 00:11:40,240
How do I manage that manually?

277
00:11:40,240 --> 00:11:42,760
That's tactic three, the clean room.

278
00:11:42,760 --> 00:11:46,960
If you have been arguing with the AI for 20 minutes and it's just going in circles, stop.

279
00:11:46,960 --> 00:11:48,320
It's gaslighting itself again.

280
00:11:48,320 --> 00:11:49,320
It is.

281
00:11:49,320 --> 00:11:51,080
You cannot fix it by arguing more.

282
00:11:51,080 --> 00:11:53,400
You're just adding more noise to the pie.

283
00:11:53,400 --> 00:11:59,200
You need to ask the AI to summarize the current solution or plan, copy that text and open

284
00:11:59,200 --> 00:12:00,680
a brand new chat window.

285
00:12:00,680 --> 00:12:02,000
Reset the brain.

286
00:12:02,000 --> 00:12:03,000
Total reboot.

287
00:12:03,000 --> 00:12:05,080
Paste the plan into the fresh window.

288
00:12:05,080 --> 00:12:09,840
You get zero dilution, zero history of errors, just the clean instruction that performance

289
00:12:09,840 --> 00:12:11,080
differences night and day.

290
00:12:11,080 --> 00:12:12,480
It takes a little more discipline though.

291
00:12:12,480 --> 00:12:14,720
You have to recognize when you're in that doom loop.

292
00:12:14,720 --> 00:12:15,720
You do.

293
00:12:15,720 --> 00:12:17,280
But that's the job now.

294
00:12:17,280 --> 00:12:19,640
We aren't prompt engineers anymore.

295
00:12:19,640 --> 00:12:21,040
We are context architects.

296
00:12:21,040 --> 00:12:23,560
You're managing the signal to noise ratio.

297
00:12:23,560 --> 00:12:26,560
There is one last thing from the article that I have to bring up because it completely broke

298
00:12:26,560 --> 00:12:27,560
my brain.

299
00:12:27,560 --> 00:12:31,440
We've been talking about this interview with the software and the insights from Zoltan

300
00:12:31,440 --> 00:12:32,800
Erdos.

301
00:12:32,800 --> 00:12:35,880
But the outro of the article reveals something.

302
00:12:35,880 --> 00:12:37,280
Well, meta.

303
00:12:37,280 --> 00:12:38,280
The research method.

304
00:12:38,280 --> 00:12:39,280
Yeah.

305
00:12:39,280 --> 00:12:42,160
This wasn't just a guy reading code and writing an article.

306
00:12:42,160 --> 00:12:46,400
He used the very method we just discussed to write the piece itself.

307
00:12:46,400 --> 00:12:49,560
He used 16 parallel AI agents.

308
00:12:49,560 --> 00:12:54,240
16, he had different agents reading the white papers, reading the documentation, reading

309
00:12:54,240 --> 00:12:58,560
the source code, and they all distilled their findings and brought them back to a central

310
00:12:58,560 --> 00:12:59,560
writer.

311
00:12:59,560 --> 00:13:01,560
It's context engineering all the way down.

312
00:13:01,560 --> 00:13:05,000
The article about context engineering was itself context engineered.

313
00:13:05,000 --> 00:13:07,880
It's fascinating because it proves the point.

314
00:13:07,880 --> 00:13:13,640
No single human and no single AI prompt could have held all that complexity and working

315
00:13:13,640 --> 00:13:14,640
memory at once.

316
00:13:14,640 --> 00:13:16,360
They had to break it down.

317
00:13:16,360 --> 00:13:17,360
That's the takeaway.

318
00:13:17,360 --> 00:13:20,760
Whether you are writing code, writing, and article, we're just trying to organize your

319
00:13:20,760 --> 00:13:21,760
life.

320
00:13:21,760 --> 00:13:25,000
Attention is a scarce resource to finite pie.

321
00:13:25,000 --> 00:13:26,680
So stop filling it with junk.

322
00:13:26,680 --> 00:13:27,680
Exactly.

323
00:13:27,680 --> 00:13:29,400
Check your context hygiene.

324
00:13:29,400 --> 00:13:32,040
Are you feeding the machine or your self-signal?

325
00:13:32,040 --> 00:13:33,720
Or are you just drowning in noise?

326
00:13:33,720 --> 00:13:36,720
Something to think about the next time you have 50 tabs open.

327
00:13:36,720 --> 00:13:38,200
Thanks for listening to The Dupdive.

328
00:13:38,200 --> 00:13:38,960
See you next time.

