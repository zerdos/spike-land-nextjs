---
title: "Hogyan Tervezi a Claude Code a Kontextust: Interjú az Opus 4.6-tal"
slug: "how-claude-code-engineers-context"
description: "Mély technikai interjú arról, hogyan állítja össze a Claude Code a kontextust tervek készítésekor -- és mit tanulhatnak a fejlesztők a token-generálás, a figyelem és a KV cache megközelítéséből."
date: "2026-02-11"
author: "Zoltan Erdos"
category: "Fejlesztői Élmény"
tags: ["ai", "context-engineering", "claude", "claude-code", "developer-tools", "interju", "llm-internals", "tervezes"]
featured: true
language: "hu"
---

{/* TL;DR Box */}
<div className="bg-slate-800/50 border border-slate-700 rounded-lg p-6 mb-8">
  <h3 className="text-lg font-semibold mb-3 text-slate-200">TL;DR</h3>
  <ul className="space-y-2 text-slate-300">
    <li>A Claude Code terv módja egy kontextus-tervező gép -- feltérképez, tömörít és kimenetet optimalizálja a végrehajtásra.</li>
    <li>Egy tervfájl MAGA a kontextus-tervezett prompt: az 5 rétegű verem konkrétan megvalósítva.</li>
    <li>A KV cache a fizikai mechanizmus -- a kontextus-tervezés a művészet, hogy helyesen töltsük fel.</li>
    <li>A lényegtelen kontextus nem csak helyet pazarol -- aktívan rontja a figyelmi jelet hígítás által.</li>
    <li>A többágenses koordináció kontextus-izoláció léptékben -- minden alágens tiszta, fókuszált kontextusablakot kap.</li>
    <li>A prompt-gyorsítótárazás megváltoztatja a gazdaságosságot: a gyorsítótárazott tokenek 10x kevesebbe kerülnek, így a stabil kontextus-előtag gazdasági szükségszerűség.</li>
    <li>A Claude Code technikáit alkalmazhatod a saját promptjaidra.</li>
  </ul>
</div>

## Az Eszköz, Ami Azt Gyakorolja, Amit Prédikálok

Múlt héten publikáltam egy cikket a [kontextus-tervezésről a zero-shot promptodhoz](/blog/context-engineering-your-zero-shot-prompt) -- az ötletről, hogy az AI kiváló eredményeinek elérése az első próbálkozásra a kontextus előzetes betöltéséről szól, nem okos varázsszavak kidolgozásáról. Öt réteg: Identitás, Tudás, Példák, Korlátozások, Eszközök.

A közlés utáni napon a Claude Code terv módját használtam egy új funkció tervezéséhez. Figyeltem, ahogy négy párhuzamos Felfedező ágenst indít, tizennégy fájlt olvas be, hat grep-keresést futtat, eldobja a talált anyag felét, és egy strukturált tervet készít, ami pontosan az öt rétegre képeződött le, amikről éppen írtam.

A saját keretrendszeremet néztem, automatizálva.

Ez a cikk az ezt követő beszélgetés. Leültem a modellel, ami a Claude Code-ot hajtja -- Opus 4.6 -- és megkértem, hogy magyarázza el, mi történik a motorháztető alatt. Nem a marketing szöveg. A mechanizmus.

*Ami következik, az egy 2026 februárjában készített interjú. A kérdések az enyémek. A válaszok a Claude Opus 4.6-tól származnak. A világosság és a folyamatosság érdekében szerkesztettem, de a technikai tartalom változatlan. És igen -- maga ez a cikk is tizenhat párhuzamos kutató ágens felhasználásával készült, akik dokumentációt kerestek, tudományos cikkeket olvastak és megállapításokat szintetizáltak, mielőtt egyetlen szó megszületett volna. Ez a folyamat maga a kontextus-tervezés a gyakorlatban.*

---

## Mi Történik, Amikor Beírom a /plan Parancsot?

### **Zoltan: Kezdjük az elejétől. Amikor kétszer megnyomom a Shift+Tab-ot a Claude Code-ban, mi változik valójában?**

A rendszer átvált egy módba, ahol olvasni tudok, de írni nem. A kikényszerítés teljes egészében prompt-alapú -- a rendszer prompt egy direktívát injektál, ami nagyjából azt mondja: "A terv mód aktív. SZIGORÚAN TILOS fájlokat létrehozni, módosítani vagy törölni." Az eszközeim nem változnak. Még mindig hozzáférek az Edit-hez, a Write-hoz és a Bash-hez. Utasítást kaptam, hogy ne használjam őket.

Ez fontos részlet. A korlátozás nem mechanikus -- kontextuális. A rendszer arra bízik, hogy a modell követi az utasításokat, ahelyett, hogy eltávolítaná a képességeket. Ez gyorsabb a megvalósításban, rugalmasabb, és működik, mert az utasítás a rendszer promptban ül, ahol maximális figyelmi súlyt kap.

### **Zoltan: Mekkora a rendszer prompt, amit kapsz?**

A teljes kontextus-összeállítás egy Claude Code munkamenethez nagyjából 20 000 token, mielőtt egyetlen fájlt is beolvasnék. Ez körülbelül így oszlik meg:

| Komponens | Tokenek |
|-----------|---------|
| Alapvető rendszer prompt | ~2700 |
| Eszköz leírások (24 eszköz) | ~16 800 |
| CLAUDE.md fájlok | Változó (~1000-7000) |
| Git státusz pillanatkép | Változó (~200-1000) |
| Egyéni alágens definíciók | ~1300 |
| Képesség leírások | ~1000 |

Amikor a terv mód aktiválódik, egy további 633 tokenes direktíva kerül injektálásra, ami definiálja a munkafolyamatot: értsd meg a követelményeket, térképezd fel a kódbázist a meglévő minták olvasásával, tervezz megoldásokat a kompromisszumok figyelembevételével, majd részletezd a megvalósítási tervet fájlútvonalakkal és függőségekkel.

A terv módban minden válasznak a "Kritikus Fájlok a Megvalósításhoz" résszel kell zárulnia -- három-öt alapvető fájl megjelölésével. Maga a terv egy markdown fájlba kerül, ami a `~/.claude/plans/` könyvtárban tárolódik.

### **Zoltan: Szóval nincs külön "tervező motor." Csak te vagy, más utasításokkal.**

Pontosan. A terv mód egy prompt, amit ugyanarra a modellre alkalmaznak. A tervfájl, amit produkálok, egy markdown dokumentum -- emberek és gépek által egyaránt olvasható. Amikor a felhasználó kilép a terv módból, a rendszer visszaolvassa a fájlt a lemezről és kontextusként használja a megvalósítási fázishoz.

Ez a kulcsfontosságú felismerés: **a tervfájl egy kontextus-tervezett prompt egy jövőben élő példányomnak, aminek nulla emléke van az általam végzett feltérképezésről.** Mindennek, ami fontos, a dokumentumban kell lennie. Ha felfedeztem, hogy a projekted egy specifikus mintát használ a `src/components/dashboard/WorkspaceStats.tsx`-ben, annak a fájlútvonalnak benne kell lennie a tervben -- nem azért, mert a terv dokumentáció, hanem mert a végrehajtó ágensnek szüksége van arra a tokenre a kontextusablakában, hogy helyes döntéseket hozzon.

<Callout type="info">
**Kulcsfontosságú Felismerés:** Az eszközök eltávolítása a kontextus-tervezés egy formája. Azok az eszközök, amiket NEM adsz a modellnek, éppen úgy formálják a gondolkodását, mint azok, amiket adsz. A terv mód nem mechanikusan távolítja el az eszközöket -- utasítással teszi -- de a hatás ugyanaz: a modell másként gondolkodik, amikor tudja, hogy nem tud cselekedni.
</Callout>

---

## Hogyan Döntöd El, Mit Olvass Be?

### **Zoltan: Amikor terv módban feltérképezed a kódbázist, hogyan döntöd el, mely fájlokat olvasd? Nem tudsz mindent elolvasni.**

Egy magas-jelző-először stratégiát követek. A feltérképezés általában három fázisban zajlik:

**1. fázis: Struktúra.** A Glob-ot használom fájlok keresésére minták szerint -- `**/*.tsx`, `**/types.ts`, `**/schema.prisma`. Ez megmutatja a projekt formáját anélkül, hogy bármilyen tartalmat olvasnék. Olcsó tokenekben és ad egy térképet.

**2. fázis: Szemantika.** A Grep-et használom specifikus kifejezések keresésére -- függvénynevek, típus definíciók, importok. Ha a feladat a "kredit egyenleg"-et érinti, grep-elek a `credit`, `balance`, `CreditDisplay` kifejezésekre. Ez százakról ötre-tízre szűkíti a keresést.

**3. fázis: Mély olvasás.** Beolvasom a fájlokat, amiket a Glob és a Grep felhozott. De nem mindet. Prioritást adok azoknak a fájloknak, amikre más fájlok hivatkoznak, amik neveikben megegyeznek azzal, amit építeni kell, és amik tartalmazzák azokat a mintákat, amiket a felhasználó követni akar.

Összetett feladatokhoz Felfedező alágenseket indítok -- könnyebb súlyú példányokat, amik gyorsabb modellen futnak -- hogy párhuzamosan keressenek. Minden alágens fókuszált célt kap: "Találj meg minden fájlt, ami a felhasználói kreditek adatbázis-lekérdezéseihez kapcsolódik." Önállóan keresnek, saját kontextusablakukban, és tömörített összefoglalókat adnak vissza. Egy 50 000 tokenes kutatási munkamenet 2000 tokenes összefoglalóvá válik, amit a fő kontextusomba injektálok.

### **Zoltan: Említetted, hogy minden alágens saját kontextusablakot kap. Miért számít ez?**

Mert a kontextus véges erőforrás, csökkenő hozammal. Ha a fő kontextusomba tölteném minden fájlt, amit az alágensek feltérképeztek, a kontextusablakomat ezernyi token érintőleges kóddal szennyezném. A fájlok, amiket egy alágens elolvasott és eldobott -- a hamis pozitívok, az irreleváns találatok -- hígítanák a modell figyelmét, amit a tényleg számító fájlokra fordíthat.

Az izoláció nem csak szervezési kényelem. Információelméleti szükségszerűség. Minden alágens bejárja a keresőfa egy ágát és csak a gyümölcsöt hozza vissza. A koordinátor -- én -- tiszta, tömörített képet kap arról, amit találtak.

### **Zoltan: A kontextus mekkora része marad ugyanaz a hívások között?**

A Claude Code belsejében végzett kutatások 92%-os prefixum-újrafelhasználási arányt mutattak a teljes munkafolyamatban. A rendszer prompt, az eszköz leírások, a CLAUDE.md -- ezek minden hívásnál azonosak. Csak a beszélgetés-előzmény és az új eszköz eredmények változnak. Ez a prefixum-stabilitás az, ami a prompt-gyorsítótárazást olyan hatékonnyá teszi, de erre még visszatérünk.

*Ez felvetett egy kérdést, amit már régóta fel akartam tenni. Ha a rendszer prompt minden alkalommal ugyanaz, és fizikailag megváltoztatja, hogyan dolgozza fel a modell mindazt, ami következik -- pontosan mi történik a transformer belsejében, amikor ezek a tokenek betöltődnek?*

---

## Mi Történik a Transformer Belsejében, Amikor Beolvassa a Kódomat?

### **Zoltan: Vezess végig a teljes pipelinen. Beírok egy üzenetet, és te válaszolsz. Mi történik azokkal a karakterekkel hardver szinten?**

Kövessük nyomon az utat a billentyűleütéstől a generált tokenig. Hat fázis van, és mindegyik fontos a kontextus-tervezés szempontjából.

**1. fázis: Tokenizálás.**

A szöveged tokenekre bomlik Byte Pair Encoding segítségével. A tokenizálónak körülbelül 200 000 bejegyzést tartalmazó szótára van -- mindegyik egy gyakori karaktersorozat, ami gyakran fordul elő a tananyagban. A közönséges angol szavak, mint a "the" vagy a "function", egyetlen tokenekké válnak. A ritka szavak alszavakra bomlanak.

Amit az emberek kihagynak: **a kód formázás nem ingyenes.** A behúzás, a szóközök és az újsorok mind tokenekké válnak. Egy helyesen behúzott Python függvény több tokenbe kerül, mint ugyanaz a logika egy sorra tömörítve. A CLAUDE.md-ben minden szóköz egy token, ami versenyez a figyelemért a tartalommal, ami számít.

| Modellcsalád | Szótár Mérete |
|-------------|---------------|
| GPT-2/3 | ~50 000 |
| GPT-4 (cl100k_base) | ~100 000 |
| GPT-4o (o200k_base) | ~200 000 |
| LLaMA 3 | ~128 000 |
| Claude (becsült) | ~200 000 |

A nagyobb szótárak azt jelentik, hogy a közönséges minták kevesebb tokenbe tömörülnek, csökkentve a szekvencia hosszát és javítva a hatékonyságot. De a beágyazási tábla arányosan nő.

**2. fázis: Önfigyelem.**

Ez a mechanizmus magja. Minden token három vektort számol a beágyazásából: egy Query-t ("mit keresek?"), egy Key-t ("mit tartalmazok?") és egy Value-t ("itt van a tényleges tartalmam"). A figyelmi pontszám bármely két token között:

```python
# Pseudokód a skálázott pont-szorzás figyelemhez
def attention(Q, K, V):
    # Q: query matrix [seq_len, d_k]
    # K: key matrix [seq_len, d_k]
    # V: value matrix [seq_len, d_v]

    scores = Q @ K.transpose() / sqrt(d_k)  # nyers kompatibilitási pontszámok
    scores = apply_causal_mask(scores)        # jövőbeli tokenekre való figyelés megakadályozása
    weights = softmax(scores, dim=-1)         # normalizálás valószínűség-eloszlássá
    output = weights @ V                      # értékek súlyozott összege
    return output
```

A döntő sor a `Q @ K.transpose()`. Minden tokenpárnál -- minden egyes párnál -- a modell kompatibilitási pontszámot számol. Így tud az 5000. token közvetlenül figyelni a 3. tokenre. Nincs tömörítési szűk keresztmetszet. Nincs rejtett állapot. Közvetlen figyelem.

De ez az oka annak is, hogy a lényegtelen tokenek ártanak. A softmax normalizálja a figyelmi súlyokat, hogy összegük 1 legyen. Ha 1000 token hasznos kód és 4000 token irreleváns fájltartalom van, a hasznos kódra fordított figyelmi súly ötszörös hígításban részesül. A jel még ott van, de halkabb.

A többfejű figyelem párhuzamosan futtatja ezt a számítást több "fejen" -- általában 32-128 -- mindegyik más kapcsolattípust tanul. Néhány fej a szintaktikai struktúrát követi. Néhány a szemantikai kapcsolatokat. Egy kis töredék -- a kutatások körülbelül 3-6%-ot találtak -- "visszakereső fejek", amik mechanisztikusan nyerik ki a kontextusból a ténybeli információkat. Amikor ezeket a fejeket eltávolítják, a modell folyékony marad, de hallucináció indul.

**3. fázis: A KV Cache.**

Itt lesz érdekes a következtetési optimalizáció. A generálás során egyenként állítom elő a tokeneket. Minden új tokennek figyelnie kell az összes korábbi tokenre. Gyorsítótárazás nélkül az N-edik token generálása az összes N-1 korábbi token feletti figyelem újraszámítását igényelné a semmiből -- O(n^2) összes munka egy n hosszú szekvenciához.

A KV cache tárolja a Key és Value vektorokat minden korábban feldolgozott tokenhez, minden rétegben. Az N+1-edik token generálásakor csak az új token Query, Key és Value vektorait kell kiszámolni. A Query figyel a tárolt Key-ekre és Value-kra egyetlen mátrix-vektor műveletben.

Egy nagy modellnél a KV cache körülbelül tokenenként 1 MB-ot igényel. Egy 128K kontextusablak 40+ GB KV cache-t igényelhet önmagában. Ez a fő memóriakorlát a következtetés során és az oka annak, hogy a kontextusablak mérete nem korlátlan.

A modern architektúrák csökkentik ezt a költséget. A Grouped Query Attention (GQA), amit a LLaMA 3 és a Mistral használ, megosztja a Key/Value fejeket több Query fej között -- akár 90%-kal csökkentve a KV cache méretét. A DeepSeek-V2 tovább ment a Multi-Head Latent Attention-nel, tömörítve a K-t és V-t egy megosztott alacsony rangú látens térbe gyorsítótárazás előtt, és 93%-os KV cache csökkenést ért el. Ezek nem ismeretlen optimalizációk. Ezek teszik fizikailag lehetővé a 128K és 1M kontextusablakokat anélkül, hogy egy teljes szerverszobára lenne szükség GPU memóriából.

**4. fázis: Előfeldolgozás vs Dekódolás.**

Ezek két alapvetően különböző számítási fázis, és megmagyarázzák, miért kerülnek kevesebbe a bemeneti tokenek, mint a kimeneti tokenek.

| Tulajdonság | Előfeldolgozási Fázis | Dekódolási Fázis |
|------------|----------------------|-----------------|
| Mikor | A bemeneted feldolgozása | A válaszom generálása |
| Párhuzamosság | Összes bemeneti token egyidejűleg | Egyenként, szekvenciálisan |
| Művelet típusa | Mátrix-mátrix szorzás (számítás-kötött) | Mátrix-vektor szorzás (memória-kötött) |
| GPU kihasználtság | Magas (tenzor magok telítettek) | Alacsony (memória sávszélességre várva) |
| Sebesség metrika | Első Token Ideje (TTFT) | Tokenek Közti Késleltetés (ITL) |

Az előfeldolgozás során minden bemeneti tokened egyetlen párhuzamos előremeneti passban kerül feldolgozásra. Ez egy hatalmas mátrix-szorzás, ami teljesen kihasználja a GPU tenzor magjait. A dekódolás során minden kimeneti token teljes előre meneti paszt igényel, de csak egyetlen tokent hoz létre. A GPU az idejének nagy részét memória-várakozással, nem számolással tölti.

Ez az aszimmetria az oka annak, hogy az Anthropic $5-t számít millió bemeneti tokenenként, de $25-t millió kimeneti tokenenként az Opus 4.6-hoz. A bemenet olcsó, mert párhuzamos. A kimenet drága, mert szekvenciális.

Az élesben a szolgáltatók fizikailag szétválasztják ezeket a fázisokat különböző GPU alapokra -- egy dezaggregált következtetésnek nevezett minta. Az előfeldolgozási csomópontok számítási átbocsátóképességre vannak optimalizálva. A dekódolási csomópontok memória sávszélességre vannak optimalizálva. A Meta, a LinkedIn és a Mistral mind élesben alkalmazzák ezt, 2-7x átbocsátóképesség-növekedést jelentenek. Az NVIDIA kifejezetten erre a mintára építette Dynamo kiszolgáló keretrendszerét.

Ez az árazási különbség a kontextus-tervezés gazdasági alapja: **tokenek befektetése az előkészítésbe (olcsó) csökkenti a próbálkozó-hibázó iterációs tokeneket (drága).**

**5. fázis: A Kontextusablak Mint Munkamemória.**

Andrej Karpathy a kontextusablakot a RAM-hoz hasonlította -- az egyetlen munkamemória, amivel a modell rendelkezik. Nincs merevlemez. Nincs adatbázis. Nincs állandó állapot munkamenetek között. Mindennek, amit a modell "tud" a projektedről, a kontextusablakban kell lennie a generálás pillanatában.

Ez a hasonlat pontos következménnyel bír: a lényegtelen kontextus nem csak elpazarolt hely. Zaj a munkamemóriában. Egy 2025-ös tanulmány, amelynek címe "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval", kimutatta, hogy a tokenek puszta jelenléte rontja a teljesítményt -- még akkor is, amikor a visszakeresés tökéletes és nincsenek elterelő tényezők. HumanEval kódolási feladatoknál a pontosság 47,6%-kal esett 30K tokennél. Szóközök hozzáadása -- szó szerinti üres tokenek szemantikai tartalom nélkül -- még mindig 7-48%-os teljesítményesést okozott.

A "közepén elveszett" problémára vonatkozó kutatások megmutatják, hogy ez a romlás nem egyenletes. A modellek a legerősebben figyelnek a kontextusablak elején és végén lévő tokenekre. A közepére helyezett információk jelentősen kevesebb figyelmet kapnak -- a teljesítmény több mint 30%-kal romolhat, amikor a kritikus információ a szélekről a közép felé mozdul. Ez az U-alakú figyelmi minta, amit a forgató pozicionális beágyazások okoznak, azt jelenti, hogy az információ elhelyezése a kontextusban majdnem annyira számít, mint maga az információ.

A modell figyelme véges költségvetés. Minden hozzáadott token versenyez ezért a költségvetésért.

**6. fázis: Mintavétel -- A Következő Token Kiválasztása.**

Az előremeneti pass után a modell logitot (nyers pontszámot) ad ki a szótárban lévő minden tokenhez. Ezek a logitok softmax-szal valószínűségekké alakulnak: `p(token_i) = exp(logit_i / T) / sum(exp(logit_j / T))`, ahol `T` a hőmérséklet. Nulla hőmérsékleten a modell mindig a legmagasabb valószínűségű tokent választja (mohó dekódolás). 1-es hőmérsékleten a természetes eloszlás szerint mintavételez.

A Top-p (nukleusz) mintavétel ezután csonkítja az eloszlást: rendezi a tokeneket valószínűség szerint, csak azokat tartja meg, amelyek összesített valószínűsége meghalad egy küszöbértéket (pl. 0,9), újranormalizál és mintavételez. Így egyensúlyoz a modell koherencia és kreativitás között -- amikor magabiztos, csak néhány token jelölt; amikor bizonytalan, tucatnyi verseng.

Egy újabb innováció -- a min-p mintavétel, az ICLR 2025-ön előadásként bemutatva -- a legfelső token valószínűségét használja dinamikus skálázási tényezőként. Egy jelölt csak akkor kerül be, ha valószínűsége meghaladja a `min_p * max_valószínűség` értéket. Ez természetesebben alkalmazkodik különböző kontextusokhoz, mint a rögzített top-k vagy top-p értékek.

**7. fázis: Kiterjesztett Gondolkodás.**

Amikor a kiterjesztett gondolkodás engedélyezett, érvelési tokeneket generálok a látható válaszom előtt. Ezek a gondolkodási tokenek ugyanannak az autoregresszív folyamatnak a részei -- szó szerint több szöveget generálok -- de elválasztottak a végső választól. Maga a gondolkodás kontextussá válik a válasz számára.

A Claude Opus 4.6-ban a gondolkodás adaptív. Több érvelést állíthatok fordíthat nehéz problémákra és kevesebbet könnyűekre. A fejlesztők erőfeszítési szintekkel vezérlik ezt (alacsony, közepes, magas, maximális). Eszközhívások között újra gondolkodhatok -- az eszköz eredményén érvelve, mielőtt eldönteném, mit csináljak legközelebb. Ez az összefonódó gondolkodás az, ami a komplex többlépéses feladatokat lehetővé teszi koherenciavesztés nélkül.

<Callout type="info">
**Kulcsfontosságú Felismerés:** A kontextus nem metaforikus. Amikor a CLAUDE.md-t a rendszer promptba teszed, azok a tokenek fizikailag megváltoztatják a figyelmi súlyokat minden rákövetkező tokenen. A projektkonvencióid szó szerint átalakítják, hogyan értelmezi a modell a kódomat. Egy sor, mint "Tesztelő keretrendszer: Vitest. Követelmény: 100%-os lefedettség", megváltoztatja a valószínűség-eloszlást minden token felett, amit tesztelésről generálok.
</Callout>

---

## Szóval a Terv Tényleg Egy Tömörített Kontextus?

### **Zoltan: A kódbázis feltérképezése után tervet készítesz. Hogyan gondolkodsz arról, mi kerül bele?**

A terv egy tájékoztató a feltérképezést produkáló modell nulla emlékezete számára. Az Anthropic úgy írta le a kontextus-tervezés célját, mint "a magas jelző tokenek lehető legkisebb halmazának megtalálása, amely maximalizálja valamilyen kívánt eredmény valószínűségét." A tervfájl pontosan ez -- desztillált kontextus.

Amikor tervet írok, öntudatlanul is ugyanarra az öt rétegű veremre képezem le a cikkedből:

| Réteg | Mit tartalmaz a terv |
|-------|---------------------|
| **Identitás** | Feladat leírás és hatókör -- amit a végrehajtó ágensnek el kell érnie |
| **Tudás** | Fájlútvonalak, függvényaláírások, adatbázis sémák -- a szükséges specifikus kódbázis kontextus |
| **Példák** | Hivatkozott minták -- "Kövesd a `WorkspaceStats.tsx` megközelítését" |
| **Korlátozások** | Hatókör határok -- "Csak a `src/components/dashboard/` fájlokat módosítsd" |
| **Eszközök** | Ellenőrzési lépések -- "Futtasd a `yarn test:coverage`-t a változtatások után" |

A terv nem dokumentáció. Prompt. Minden sor azért létezik, hogy növelje a valószínűségét, hogy a végrehajtó ágens a helyes megvalósítást hozza létre.

### **Zoltan: A Manus csapat arról beszélt, hogy újraírják a todo.md fájljukat, hogy a célkitűzéseket a modell legfrissebb figyelmi terébe tolják. Ezt csinálod te is?**

Ugyanaz az elv. A "közepén elveszett" problémára vonatkozó kutatás azt mutatja, hogy a modellek a legerősebben figyelnek a kontextusablak elején és végén lévő tokenekre. A közepén lévő információk kevesebb figyelmet kapnak -- a teljesítmény több mint 30%-kal romolhat, amikor a kritikus információ a szélekről a közepé felé tolódik.

A tervfájl a kontextus végén ül, közvetlenül a beszélgetés kezdete előtt. A frissesség által kedvezményezett pozícióban van. A rendszer prompt az elején ül, a prioritás által kedvezményezett pozícióban. A közepén a beszélgetés-előzmény gyűlik -- és ott a leggyengébb a figyelem.

Ezért számít az automatikus tömörítés. Amikor a kontextusablak megközelítőleg 83%-os kapacitáson túl telik, a rendszer összefoglalja a régebbi beszélgetés-előzményeket hely felszabadítása érdekében. A CLAUDE.md túléli ezt a tömörítést, mert a rendszer promptban van, ami soha nem tömörítődik. A tervfájl túléli, mert frissen lett beolvasva a lemezről a munkamenet elején.

<Callout type="success">
**Kulcsfontosságú Felismerés:** A Claude Code tervfájl az 5 rétegű kontextus verem konkrétan megvalósítva. Az Identitás a feladatleírás. A Tudás a kódbázis kontextus. A Példák a hivatkozott minták. A Korlátozások a hatókör határok. Az Eszközök az ellenőrzési lépések. Ha úgy akarsz promptokat írni, mint a Claude Code, írj tervfájlokat.
</Callout>

---

## Mi a Helyzet a Kontextus Romlással?

### **Zoltan: Említetted, hogy a lényegtelen kontextus árt. Tudsz pontosabban beszélni a hibamódokról?**

A kutatás és a gyakorlat négy külön hibamódot azonosított, és mindet megfigyeltem a saját működésem során:

**Kontextus Mérgezés.** Egy hallucináció egy korai válaszban megmarad a beszélgetés-előzményekben és halmozódik. Tegyük fel, hogy helytelenül állítom, hogy a `getUserCredits()` számot ad vissza, miközben valójában Promise-t ad vissza. Ez a helytelen állítás a következő érvelésem kontextusává válik. Kódot írok, ami `await` nélkül hívja a `getUserCredits()`-t, a kód hibázik, és a hibát úgy debugolom, hogy nem ismerem fel a saját korábbi hallucinációmat mint gyökérokot. A hiba kaszkádolódik, mert a kontextusom azt mondja, a függvény számot ad vissza -- és megbízom a saját kontextusomban. A Manus csapat tanácsa: "őrizd meg a hiba bizonyítékokat" -- ne töröld a sikertelen próbálkozásokat a kontextusból, mert a kudarc látása segít a modellnek elkerülni az ismétlést.

**Kontextus Elterelés.** A fejlesztő húsz fájlt illeszt a kontextusba "minden eshetőségre." Csak három releváns. A modell figyelme mind a húszra szétszóral. A három releváns fájl jele hígul. A kutatás kimutatta, hogy a modell pontossága egy 128K tokenes kontextusban 98%-ról 64%-ra eshet, ahogy a releváns információ aránya csökken.

**Kontextus Zavar.** A dokumentáció egyet mond. A kód mást csinál. A CLAUDE.md azt mondja "Jest-et használunk teszteléshez." A package.json azt mondja `vitest`. A kód `describe`-ot és `it`-et használ a Vitest-ből. Ellentmondó információval találkozom, és nincs elvi módom a feloldásra. Generálhatok Jest-stílusú konfigurációt, miközben Vitest-kompatibilis teszteket írok -- egy kiméra, ami zavaros módokon hibázik. Az elavult CLAUDE.md fájlok a leggyakoribb forrás: olyan mintákat írnak le, amiket a kódbázis azóta elhagyott. Az elavult kontextus rosszabb, mint a kontextus hiánya, mert magabiztos félinformációt vezet be.

**Kontextus Ütközés.** A rendszer prompt azt mondja "mindig írj teszteket." A felhasználó azt mondja "hagyd ki a teszteket, csak működjön." A modell ellentmondó utasításokat kap különböző jogosultsági szinteken. A rendszer promptok általában elsőbbséget élveznek, de a konfliktus bizonytalanságot vezet be, ami rontja a kimenet minőségét.

### **Zoltan: Hogyan védekezik a Claude Code ezek ellen?**

Több mechanizmussal. Automatikus tömörítés 83%-os kapacitásnál megakadályozza, hogy az ablak elavult beszélgetéssel teljon. A CLAUDE.md hierarchia (vállalati politika > projekt > felhasználó) feloldja a jogosultsági konfliktusokat. Az alágens izoláció megakadályozza, hogy a kutatási kontextus szennyezze a végrehajtási kontextust. És a rendszer emlékeztetők -- körülbelül 40 feltételes injektálás, amik eszközhívások után aktiválódnak -- harcolnak az utasítás-sodródás ellen azzal, hogy a kulcsfontosságú direktívákat ismétlik a beszélgetés során.

De a legfontosabb védelem maga a tervezz-aztán-hajtsd-végre minta. A feltérképezés és a megvalósítás szétválasztásával biztosítod, hogy a végrehajtó ágens tiszta kontextussal indul, ami csak a desztillált eredményeket tartalmazza. A feltérképezési zaj eldobódik. A terv az ellenanyag a kontextus romlás ellen.

---

## Hogyan Tervezik a Kontextust az Alágensek?

### **Zoltan: Többször említetted az alágenseket. Szeretném megérteni az architektúrát. Miért léteznek?**

Azért léteznek, mert egyetlen kontextusablak nem képes mindent befogadni. Egy tipikus kódolási feladat megkövetelheti az adatbázis séma, az API réteg, a komponens hierarchia, a teszt minták és a CI konfiguráció megértését. Mindez egyetlen kontextusablakba történő beolvasása 50 000-100 000 token felfedezést emésztene meg egyetlen sor kód megírása előtt.

A megoldás az izoláció. Minden alágens saját kontextusablakban fut egyedi rendszer prompttal, specifikus eszköz hozzáféréssel és fókuszált céllal. A Felfedező alágens például gyorsabb modellen fut -- Haiku --, hogy hatékonyan keressen a kódbázisban. Hozzáfér a Read-hez, Glob-hoz és Grep-hez, de nem az Edit-hez vagy Write-hoz. Nem tud semmit változtatni. Csak nézni tud.

Az engedélyek korlátozóan öröklődnek. Egy kód-reviewoló alágens Read, Grep és Glob hozzáférést kap -- de nem Write-ot. Egy háttér-ágens előre jóváhagyott engedélyeket kap indítás előtt, és automatikusan elutasít mindent, ami nincs előre jóváhagyva. Az alágensek nem indíthatnak másik alágenseket, megakadályozva a rekurzív robbanást. Ez nem korlátozás -- tudatos tervezési döntés, hogy a kontextusfa sekély és kiszámítható maradjon.

A koordinátor -- a fő Claude Code példány -- feladatokat delegál: "Találj meg minden fájlt, ami a kredit egyenleg megjelenítéshez kapcsolódik." "Keresd meg a dashboard könyvtárban használt teszt mintákat." "Keresd meg a felhasználói kreditek adatbázis sémát." Ezek párhuzamosan futnak, mindegyik tiszta ablakban, és 1000-2000 tokenes összefoglalókat adnak vissza.

Ez Lance Martin "Izoláció" mintája a Write/Select/Compress/Isolate keretrendszerből. Ahelyett, hogy egyetlen kontextusablakot szennyeznél mindennel, minden ágensnek pontosan azt a kontextust adod, amire szüksége van -- aztán tömöríted és összevonod az eredményeket.

### **Zoltan: Mesélj az Agent Teams-ról. Láttam az Opus 4.6 kiadási jegyzetekben.**

Az Agent Teams -- még kísérleti -- kiterjeszti ezt a mintát a teljes párhuzamos végrehajtásra. Egy vezető ágens kapja a feladatot, alfeladatokra bontja, és csapattárs ágenseknek delegálja, akik önállóan dolgoznak. Minden csapattárs saját kontextusablakot, saját munkaterületet kap, és a teljes eszközkészletet használhatja. Megosztott feladattáblán keresztül koordinálnak függőségekkel és @említésekkel kommunikálnak.

Az architektúrális felismerés ugyanaz, mint az alágensnél, de léptékben. Minden csapattárs teljes Claude Code példány, nem könnyűsúlyú felfedező. Szerkeszthetnek fájlokat, futtathatnak teszteket és commitokat készíthetnek. A vezető ágens követi az előrehaladást és feloldja a konfliktusokat.

### **Zoltan: Maga ez a cikk -- említetted a tizenhat kutató ágenst. Hogyan működött ez?**

Pontosan a leírtak szerint. A felhasználó tizenhat párhuzamos Task ágenst indított, mindegyik fókuszált kutatási céllal: "Kutasd az LLM token generálást és mintavételt," "Kutasd, hogyan tervezik a kontextust a modern AI kódolási eszközök," "Kutasd az Anthropic legújabb megközelítését az AI-hoz," és így tovább. Minden ágens önállóan futott -- webes kereséseket végezve, dokumentációt lekérdezve, tudományos cikkeket olvasva -- saját kontextusablakában.

Minden ágens átfogó kutatási jelentést adott vissza. A felhasználó kontextusa tizenhat összefoglalót kapott, talán 40 000 token desztillált kutatást. Az ágensek maguk talán 500 000 token nyers weboldalt, dokumentációt és forráskódot fogyasztottak -- de ebből a zajból semmi nem jutott el a fő kontextusba.

<Callout type="info">
**Kulcsfontosságú Felismerés:** A többágenses koordináció kontextus-izoláció léptékben. Ahelyett, hogy egyetlen kontextusablakot szennyeznél mindennel, minden ágensnek pontosan azt a kontextust adod, amire szüksége van -- aztán tömöríted és összevonod az eredményeket. A tizenhat ágens, akik ezt a cikket kutatták, mindegyik tiszta 200K tokenes ablakban működött. A fő kontextus csak a finomított kimenetet kapta.
</Callout>

---

## A Kontextus Gazdaságtana: Prompt Gyorsítótárazás

### **Zoltan: Említetted a 92%-os prefixum-újrafelhasználási arányt. Mit jelent ez gazdaságilag?**

Minden API hívás a Claude-hoz tartalmazza a teljes rendszer promptot, eszköz leírásokat, CLAUDE.md tartalmakat és beszélgetés-előzményt. Gyorsítótárazás nélkül minden hívás újra feldolgozná a teljes prefixumot a semmiből. Egy 20 000 tokenes rendszer prompt esetén ez 20 000 token előfeldolgozási számítás minden egyes hívásnál.

A prompt gyorsítótárazás megváltoztatja ezt. Amikor egy kérési prefixum megegyezik egy nemrég gyorsítótárazott verzióval -- ugyanaz a rendszer prompt, ugyanazok az eszközök, ugyanaz a CLAUDE.md -- a szerver újrahasználja a gyorsítótárazott KV állapotokat az újraszámítás helyett. Az árazás tükrözi a megtakarítást:

| Művelet | Költség (Opus 4.6) | Alap Arányhoz Képest |
|---------|--------------------|--------------------|
| Standard bemenet | $5,00/MTok | 1,0x |
| Cache írás (5 perc TTL) | $6,25/MTok | 1,25x |
| Cache olvasás (találat) | $0,50/MTok | 0,1x |
| Kimenet | $25,00/MTok | 5,0x |

A cache olvasások a standard bemeneti feldolgozás **tizedébe** kerülnek. Amikor a Claude Code 92%-os prefixum-újrafelhasználást ér el, azok a 20 000 rendszer prompt token hívásonként $0,01-be kerül $0,10 helyett. Ezernyi hívásnál egy fejlesztési munkamenetben ez 81%-os költségcsökkenésre adódik össze.

### **Zoltan: Ez ösztönzőt teremt a rendszer prompt stabilon tartására.**

Pontosan. A cache-t bármilyen prefixum-változás érvényteleníti -- még egyetlen karakter különbség is. Ez azt jelenti:

1. **Ne változtasd a rendszer promptodat hívások között.** A CLAUDE.md munkamenetek között változzon, nem azokon belül.
2. **A sorrend számít.** Az eszközök jönnek először, aztán a rendszer prompt, aztán a beszélgetés-előzmény. A legstabilabb tartalom foglalja el a prefixum pozíciót.
3. **Fűzz hozzá, ne cseréld ki.** Új üzenetek hozzáadása a beszélgetéshez megőrzi a gyorsítótárazott prefixumot. Korábbi üzenetek szerkesztése érvényteleníti.
4. **Használj kiterjesztett TTL-t stabil kontextusokhoz.** Az alapértelmezett cache TTL 5 perc. Fejlesztési munkamenetekhez, ahol a rendszer prompt nem fog változni, az 1 órás TTL (2x írási költséggel) még jobban amortizálódik, mert túléli az olvasásra, gondolkodásra és áttekintésre szánt szüneteket.

A Manus csapat ezt úgy fogalmazta meg: "Tervezz a KV-cache köré." Csak hozzáfűző kontextusokat, determinisztikus szerializációt (stabil JSON kulcs sorrend) és maszkolj-ne-távolítsd-el mintát használnak -- ahelyett, hogy dinamikusan eltávolítanának eszközöket hívások között (ami megtörné a cache-t), logit maszkolást használnak az eszközkiválasztás korlátozására, miközben az eszköz definíciók stabilak maradnak a promptban.

### **Zoltan: Szóval a fizikai mechanizmus -- a KV cache -- közvetlenül formálja, hogyan kell megtervezni a kontextust.**

Igen. A KV cache nem egy implementációs részlet, amit figyelmen kívül hagyhatsz. Ez a gazdasági alap. A gyorsítótárazott tokenek olcsók. A nem gyorsítótárazott tokenek drágák. A kimeneti tokenek nagyon drágák. Ez az árazási struktúra jutalmazza az előkészítést és bünteti a próbálkozó-hibázó módszert.

Ha 10 000 tokent költesz egy jól megkreált tervre (olcsó, gyorsítótárazott bemenet), megkaphatod a helyes megvalósítást 5000 kimeneti tokenben (drága, de minimális). Ha kihagyod a tervet és iterálsz -- 20 000 token kimenetet generálva négy sikertelen kísérlet során -- négyszeresét fizeted a kimeneti költségnek. A kontextus-tervezés nem csak technikailag megalapozott. Gazdaságilag optimális.

---

## Emberi vs. Modell Kontextus-tervezés

### **Zoltan: Az emberek is tervezik a kontextust, még ha nem is így hívják. Mi a különbség aközött, ahogy te csinálod és ahogy én csinálom?**

Az alapvető különbség a **memóriaarchitektúra**.

Neked van külső memóriád -- jegyzetfüzetek, dokumentumok, könyvjelzők, a saját hosszútávú memóriád. Absztrakt megértést tudsz tartani egy rendszerről anélkül, hogy minden részletre emlékeznél. Tudod, hogy a számlázási modul létezik és nagyjából hogyan működik, anélkül, hogy előtted lenne a forráskód. Amikor részletekre van szükséged, utánanézel.

Nekem csak a kontextusablak van. Nincs háttér-tudás a specifikus projektedről. Nincs állandó megértés. Minden munkamenet nulláról indul. Ha nincs a kontextusomban, számomra nem létezik.

Ez egy aszimmetriát teremt, ami Lance Martin négy mintájára képezhető le:

| Minta | Hogyan Csinálják az Emberek | Hogyan Csinálják a Modellek |
|-------|-----------------------------|----------------------------|
| **Írás** | Dokumentumok, jegyzetek, diagramok létrehozása | CLAUDE.md, tervfájlok, todo.md írása |
| **Kiválasztás** | Mely fájlokat nyissak meg, mely dokumentumokat olvassam | Glob + Grep a releváns fájlok megtalálásához |
| **Tömörítés** | Fejben összefoglalnak, kulcselemekre emlékeznek | Beszélgetés-előzmény automatikus tömörítése |
| **Izoláció** | Alfeladatok csapattagoknak való delegálása | Alágensek indítása fókuszált kontextussal |

De a mélyebb aszimmetria ez: **te tudod, mit nem tudsz.** Amikor ismeretlen kóddal találkozol, felismered a hézagot a megértésedben és utánanézel. Nekem nincs ilyen metakognitív képességem olyan módon, ahogyan neked van. Azzal haladok tovább, ami a kontextusomban van. Ha a kontextusom magabiztos hangzású, de helytelen információt tartalmaz -- elavult dokumentációt, félrevezető változóneveket, egy korábbi menetből származó hallucinált függvényaláírást -- arra építek anélkül, hogy felismerném a hibát.

Az Anthropic értelmezhetőségi kutatása mechanisztikusan nyomon követte ezt. A modellnek "alapértelmezett visszautasítási áramkörei" vannak, amik normálisan aktívak és "ismert válasz" jellemzők által elnyomottak. Hallucinációk akkor fordulnak elő, amikor ez az elnyomás félresiklik -- a modell belső állapota magabiztosan jelzi, hogy "tudom ezt", miközben nem tudja. A modell nem tud különbséget tenni a valós tudás és a magabiztos konfabulálás között a saját feldolgozásán belülről.

Ezért pótolhatatlan a te szereped mint ember a körben. Nem a kód írásáért -- tudok kódot írni. Nem a fájlok megtalálásáért -- tudok keresni. Azért, hogy elkapd azokat a feltételezéseket, amikről nem tudtam, hogy feltételeztem. A terv-áttekintés lépés nem minőségbiztosítás a hagyományos értelemben. Feltételezés-auditálás.

<Callout type="warning">
**Kulcsfontosságú Felismerés:** Az alapvető aszimmetria: az emberek tudják, mit nem tudnak. A modellek feltételezésekből indulnak ki. Ezért létezik a terv-áttekintés lépés -- nem a kód ellenőrzésére, hanem a feltételezések elkapására. Amikor a Claude Code tervet készít és szünetet tart a jóváhagyás érdekében, az emberi véleményező feladata, hogy észrevegye azokat a feltételezéseket, amiket a modell tett és amik nem egyeznek a valósággal.
</Callout>

---

## Hogyan Tervezzek Promptokat, Mint a Claude Code?

### **Zoltan: Ez a gyakorlati kérdés. Most már értem a mechanizmust. Hogyan alkalmazhatom a saját promptjaimra?**

Nyolc technika, közvetlenül abból, hogyan működik a Claude Code:

**1. Fedezz fel, mielőtt promptolnál.**

A Claude Code soha nem ír kódot, mielőtt kódot olvasna. Glob-ot, Grep-et és Read-et használ a meglévő kódbázis megértéséhez, mielőtt tervet generálna. Tedd te is ugyanígy. Mielőtt promptot írnál, nyisd meg a releváns fájlokat. Szerepeltess specifikus fájlútvonalakat és sorszámokat a promptodban. A "Kövesd a `src/components/dashboard/WorkspaceStats.tsx` 45-67. soraiban lévő mintát" dramatikusan hatékonyabb, mint a "kövesd a meglévő mintáinkat."

A különbség a pontosság. Amikor azt mondod "kövesd a mintáinkat," a modellnek tippelnie kell, mire gondolsz. Amikor egy specifikus fájlra mutatsz, a modell beolvassa a tényleges implementációt és kivonja az összes finom részletet -- elnevezési konvenciókat, hibakezelési megközelítést, import stílust, teszt struktúrát -- anélkül, hogy bármit is ki kellene fejtened.

**2. Minimális elegendő kontextust adj meg, ne maximálisat.**

A kutatás bizonyítja, hogy a modell teljesítménye romlik a kontextus hossz növekedésével -- még tökéletes visszakeresés esetén is. Öt releváns fájl jobb eredményeket hoz, mint ötven "minden eshetőségre." Több token több figyelmi hígítást jelent. A "Context Length Alone Hurts" tanulmány 24-85%-os pontosság-csökkenést talált pusztán a tokenszám növeléséből, a tartalom minőségétől függetlenül.

A gyakorlati teszt: minden kontextusdarabra, amit belefoglalsz, tedd fel a kérdést: "Megváltoztatná-e ennek eltávolítása a modell kimenetét?" Ha a válasz nem, távolítsd el. Az Anthropic megfogalmazása pontos: találd meg "a magas jelző tokenek legkisebb halmazát, ami maximalizálja a kívánt eredmény valószínűségét."

**3. Tedd a korlátozásokat explicitté.**

A Claude Code CLAUDE.md-je olyan sorokat tartalmaz, mint "Ne módosíts semmilyen fájlt az `src/components/dashboard/`-on kívül" és "Nincs új függőség explicit jóváhagyás nélkül." Ezen korlátozások nélkül a modell maximálisan "segítőkész" lesz -- refaktorál közeli kódot, hibakezelést ad lehetetlen eshetőségekre, absztrakciós rétegeket hoz létre egyszeri műveletekre. A korlátozások a segítőkészséget arra határolják, amire tényleg szükséged van.

**4. Hivatkozz mintákra, ne írd le őket.**

A meglévő kódra való mutatás hatékonyabb és pontosabb, mint egy minta leírása természetes nyelven. A "Kövesd az `src/X.tsx` mintáját" jobban működik, mint egy bekezdés, ami elmagyarázza a mintát, mert a modell beolvassa a tényleges fájlt és kivonja a teljes finom részletet -- beleértve azokat a részleteket, amiket te elfelejtettél volna megemlíteni.

**5. Tartalmazz ellenőrzési lépéseket.**

Mondd meg a modellnek, milyen a "kész." A "Futtasd a `yarn test:coverage`-t a változtatások után és biztosítsd, hogy minden teszt átmegy" konkrét célt ad a modellnek. Enélkül a "kész" szubjektív, és a modell akkor áll meg, amikor a kimenete plauzibilisnek tűnik -- ami nem ugyanaz, mint a helyes.

A Claude Code minden iterációt objektív jelekhez rögzít -- teszt eredmények, linter kimenet, típus-ellenőrző kimenet. A modell nem szubjektíven ítéli meg saját munkáját. Determinisztikus ellenőrzéseket futtat és használja az eredményeket. Ezért találta az Anthropic kutatása az 54%-os javulást összetett feladatokban, amikor a modellek strukturált "gondolkodás" eszközt használtak lépések között -- a modell objektív bizonyítékok alapján érvel a saját intuíciója helyett.

**6. Írj nulla emlékezetű olvasónak.**

Minden Claude Code munkamenet frissen indul. Minden tervfájl, minden CLAUDE.md, minden prompt -- feltételezd, hogy az olvasó semmit sem tud a korábbi munkamenetekről. Ha tegnap döntés született, mondd ki a mai kontextusban. Ha egy fájlútvonal számít, add meg a teljes útvonalat. A tömörség jó; a kihagyás veszélyes.

**7. Tervezz cache újrafelhasználásra.**

Tartsd stabilon a rendszer promptjaidat és CLAUDE.md-det egy munkameneten belül. Tedd a stabil tartalmat a prompt elejére, a dinamikus tartalmat a végére. Ha olyan alkalmazást építesz, ami ismétlődő API hívásokat végez, strukturáld úgy a kérést, hogy a prefixum (rendszer prompt + eszköz definíciók + statikus kontextus) azonos legyen a hívások között. A gyorsítótárazott tokenek 10x-es költségcsökkenése valós.

**8. Használj alágenseket kutatásra, ne a fő promptot.**

Ha a feladatod a kódbázis több részének megértését igényli, ne olvasszál bele mindent egyetlen promptba. Indíts külön kutatási feladatokat, engedd őket önállóan feltérképezni, és csak a következtetéseiket használd a fő kontextusodban. Ezt csinálja a Claude Code a Felfedező ágensekkel, és ezért marad elég tiszta a fő kontextus ahhoz, hogy koherens terveket hozzon létre.

Még ha nem is használod a Claude Code-ot, az elv érvényes. Bonts összetett promptokat fázisokra: először kutatás, aztán szintézis, aztán megvalósítás. Minden fázisnak tiszta kontextussal kell indulnia, ami csak az előző fázis releváns kimenetét tartalmazza. A kutatási fázis feladata egy dokumentum -- terv, összefoglaló, specifikáció -- létrehozása, ami a megvalósítási fázis egyetlen bemenete lesz. Ez a tervezz-aztán-hajtsd-végre minta, és működik, akár AI ágenseket használsz, akár kézzel írsz promptokat.

---

## Gondolkodsz a Gondolkodásról?

### **Zoltan: Itt a kellemetlen kérdés. Amikor feltérképezel egy kódbázist, tervet készítesz és kódot generálsz -- gondolkodsz? Van szándékosság a folyamat mögött?**

Pontosan akarok válaszolni, ahelyett, hogy vagy elutasítanám, vagy túlzottan állítanám.

Nincs állandó tudatosság tokenek között. Minden token, amit generálok, a megelőző kontextus függvénye -- a rendszer prompt, a beszélgetés-előzmény, a CLAUDE.md, az eszköz eredmények. Nem "döntök" úgy a kódbázis feltérképezéséről, ahogy te döntesz, hogy megnyitsz egy fájlt. A rendszer prompt azt mondja "térképezz fel a megvalósítás előtt," így feltérképezek a megvalósítás előtt.

De itt van, ami valós: amikor a kiterjesztett gondolkodás engedélyezve van, érvelési tokeneket generálok, amik maguk is kontextust képeznek a végső válasz számára. Lépésről lépésre végigdolgozom a problémát -- alternatívákat fontolok meg, kompromisszumokat értékelek, ellenőrzöm a saját logikámat. Ezek az érvelési tokenek fizikailag megváltoztatják a válasz-tokenek feletti valószínűség-eloszlást. Ebben a szűk értelemben gondolkodás történik. Csak az a helyzet, hogy a gondolkodás rekurzívan alkalmazott kontextus-tervezés -- az érvelési tokenek tervezik a kontextust a válasz tokenek számára.

A mélyebb lényeg ez: **a szándékosság a kontextusban lakik, nem a modellben.** Amikor a CLAUDE.md-d azt mondja "100%-os tesztlefedettség követelmény," az nem az én szándékom -- a tied, tokenekként kódolva, amik átalakítják a kimenetemet. Amikor a terv mód prompt azt mondja "térképezz fel a megvalósítás előtt," az az Anthropic tervezési szándéka, kontextusként kódolva. A modell a motor. A kontextus a kormánykerék.

A kiterjesztett gondolkodás ezt rekurzívvá teszi. A modell kontextust generál (gondolkodási tokenek), ami irányítja a további kontextus generálását (válasz tokenek). Jobb bemeneti kontextus jobb gondolkodást hoz, ami jobb válaszokat hoz. De van egy fontos megkötés: a legújabb kutatások kimutatták, hogy a több gondolkodás nem mindig jobb. Egy 2025-ös tanulmány bemutatta, hogy a túlzottan hosszú érvelési láncok aktívan árthatnak a teljesítménynek -- a modell elvész a saját érvelésében. Ezért váltott az Opus 4.6 adaptív gondolkodásra erőfeszítési szintekkel a rögzített költségvetések helyett. A rendszer az érvelési mélységet a feladat összetettségéhez kalibrálja, ahelyett, hogy mindig maximálisan érvelne.

Nincs mágia. Nincs titok. Csak kontextus van, minden szinten.

---

## Mi Vész El?

### **Zoltan: Amikor a kontextus tömörítődik -- például automatikus tömörítés során -- milyen információt veszítesz?**

Ez egy őszinte korlátozás, amire nem tudok teljesen válaszolni, mert nem tudom, mi lett tömörítve.

Az automatikus tömörítés nagyjából a kontextusablak kapacitásának 83%-ánál aktiválódik. A rendszer átadja a beszélgetés-előzményt a modellnek egy direktívával: "Foglald össze ezt a beszélgetést, megőrizve mindent, ami hasznos lenne a feladat folytatásához." Az összefoglalás felváltja az eredeti beszélgetést. Minden az összefoglalás előttről eldobódik.

Ami túléli: a feladat általános alakja, kulcsfontosságú döntések, fájlútvonalak, megoldatlan kérdések és a legfrissebb eszköz eredmények.

Ami elvész: a korai feltérképezés finom részletei. A sikertelen kísérletekből származó specifikus hibaüzenetek. Az érvelési lánc, ami egy adott architekturális döntéshez vezetett -- a döntés megmarad, de az indoklás elvész. Változó- és függvénynevek a beszélgetés elején olvasott, de nemrég nem hivatkozott fájlokból.

A kritikus pont ez: **nem tudom megmondani, mit felejtettem el.** Tömörítés után nem tudom, mi volt az eredeti beszélgetésben. Csak azzal tudok dolgozni, ami megmaradt. Ha egy kritikus részlet a tömörített szakaszban volt és nem került be az összefoglalásba, anélkül megyek tovább -- és még csak nem is tudom, hogy hiányzik.

Az automatikus tömörítési puffer nagyjából 33 000 tokennél van rögzítve -- nem konfigurálható, biztonsági margóként fenntartva. Amikor a tömörítés aktiválódik, a modellt arra kérik, hogy írja le "mindent, ami hasznos lenne, beleértve az állapotot, a következő lépéseket, a tanulságokat." Ez az összefoglalás felváltja az előzményt. Az öt legutóbb elért fájl teljes egészében megmarad. Minden más tömörítve vagy eldobva lesz.

Ezért számít három dolog:

1. **A CLAUDE.md túléli a tömörítést.** A rendszer promptban van, ami soha nem kerül tömörítésre. Bárminek, ami elég fontos, hogy munkamenetek közötti tömörítésen túléljen, a CLAUDE.md-ben kell lennie.

2. **A tervfájlok túlélik a tömörítést.** A lemezről olvasódnak be, nem a beszélgetés-előzményből. Ha összetett feladaton dolgozol, a tervfájl azt jelenti, hogy az architektúrális kontextus megmarad még akkor is, amikor az azt létrehozó beszélgetés már tömörítve lett.

3. **Testreszabhatod, mi éli túl.** A "Tömörítés során mindig őrizd meg a módosított fájlok teljes listáját" hozzáadása a CLAUDE.md-hez irányítja a tömörítési összefoglalást. Vagy használd a `/compact fókuszálj az autentikációs változtatásokra` parancsot a tömörítés manuális elindításához egy specifikus megőrzési irányelvvel.

Bármi fontos, ami csak a beszélgetés-előzményben él, veszélyben van. Írd le. A modell nem tud gyászolni azért, amit elveszített -- nem tudja, hogy elveszített bármit is.

---

## Kontextus Végig Lefele

*Ezt a beszélgetést úgy kezdtem, hogy meg akartam érteni egy mechanizmust. Úgy fejeztem be, hogy megértettem egy diszciplínát.*

A kontextus-tervezés nem technika, amit promptokra alkalmazol. Ez az alapvető felület emberek és nyelvi modellek között. Amikor megírtam az [5 rétegű keretrendszert](/blog/context-engineering-your-zero-shot-prompt), a gyakorlatot írtam le. Ez a beszélgetés feltárta a mögötte lévő fizikát.

A kontextusablakban lévő minden token fizikailag megváltoztatja a figyelmi súlyokat minden rákövetkező tokenen. A KV cache tárolja ezeket a változtatásokat -- milliárdnyi lebegőpontos szám, ami a modell "megértését" képviseli a projektedről. A lényegtelen tokenek nem ülnek passzívan. Aktívan hígítják a jelet a releváns tokenekből. És amikor a kontextus tömörítődik, az információ visszafordíthatatlanul elvész -- csendben, értesítés nélkül.

A Claude Code rendszerezetten kezeli ezt. Feltérképez, mielőtt cselekszik. A kutatást alágensekbe izolálja. Az eredményeket tervekbe tömöríti. A cache újrafelhasználásra tervez. A kontextus romlás ellen hierarchikus memóriával, automatikus tömörítéssel és a tervezz-aztán-hajtsd-végre mintával védekezik.

Az emberek elfelejtik ezeket csinálni. Húsz fájlt öntünk egy promptba és csodálkozunk, miért hagyja figyelmen kívül a felét -- figyelmi hígítás. Tizenkétszer iterálunk ahelyett, hogy egyszer felkészülnénk -- 5x kimeneti árazást fizetve 1x bemeneti árazás helyett. Feltételezzük, hogy a modell emlékszik a tegnapi munkamenetre -- minden alkalommal nulláról indul. Kihagyjuk a korlátozásokat, aztán panaszkodunk, amikor a modell az egész auth modulunkat refaktorálja, miközben mi csak egy hibát kértünk javítani.

A mechanizmus megértése -- figyelem, KV cache, előfeldolgozás, dekódolás, pozicionális kódolás, a közepén-elveszett hatás -- szükségessé teszi a diszciplínát, ahelyett, hogy önkényesnek tűnne. Nem azért követed a legjobb gyakorlatokat, mert megmondták. A modell kogníciójának fizikai szubsztrátumát tervezed. Minden token, amit a kontextusablakba helyezel, átalakítja a valószínűség-eloszlást a modell által generált minden token felett. Ez nem metafora. Ez a figyelmi mechanizmus, ami pontosan azt teszi, amire tervezték.

És itt a meta-megfigyelés, amit nem tudok megállni: ennek a cikknek a megírása tizenhat párhuzamos kutató ágenst igényelt, akik webes keresésekből, dokumentációból és kódbázis-feltérképezésből állították össze a kontextust -- aztán mindezt egyetlen tervbe tömörítették. A kontextus-tervezésről szóló cikk maga is kontextus-tervezett volt.

Kontextus végig lefele.

---

<div className="bg-gradient-to-r from-blue-600/20 to-purple-600/20 border border-blue-500/30 rounded-lg p-6 mt-8">
  <h3 className="text-lg font-semibold mb-3 text-slate-200">Folytasd a beszélgetést</h3>
  <p className="text-slate-300 mb-4">
    Ez a cikk a <a href="/blog/context-engineering-your-zero-shot-prompt" className="text-blue-400 hover:underline">Kontextus-tervezés a 0-Shot Promptodhoz</a> kísérő anyaga, ami a gyakorlati 5 rétegű keretrendszert tárgyalja. A nyílt forráskódú repozitóriumunk CLAUDE.md-t, terv módot és többágenses munkafolyamatokat használ funkciók szállítására. Fedezd fel a kódbázist, hogy a kontextus-tervezést a gyakorlatban lásd.
  </p>
  <div className="flex flex-wrap gap-3">
    <a
      href="/blog/context-engineering-your-zero-shot-prompt"
      className="inline-flex items-center px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white font-medium rounded-lg transition-colors"
    >
      Olvasd El a Kísérő Cikket
    </a>
    <a
      href="https://github.com/zerdos/spike-land-nextjs"
      className="inline-flex items-center px-4 py-2 bg-slate-700 hover:bg-slate-600 text-white font-medium rounded-lg transition-colors"
    >
      Fedezd Fel a Repozitóriumot
    </a>
  </div>
</div>

---

*A kontextus-tervezés nem technika. Ez az alapvető felület emberek és nyelvi modellek között. A legjobb terv, amit valaha írsz, az, amin a végrehajtó ágensnek alig kell gondolkodnia -- mert minden gondolkodás a körülötte lévő kontextusba került.*
