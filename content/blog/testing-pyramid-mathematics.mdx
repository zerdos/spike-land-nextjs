---
title: "The Testing Pyramid Is Upside Down"
slug: "prime-factorization-of-your-test-suite"
description: "Every integer has a unique prime factorization. So does every user flow. Browser tests verify composites — multiplying business logic with rendering, timing, and DOM. You cannot reliably test primes by testing their products. MCP decomposes the composite back into testable primes."
date: "2026-02-13"
author: "Zoltan Erdos"
category: "Developer Experience"
tags: ["testing", "mcp", "mathematics", "prime-factorization", "dimensional-reduction"]
featured: false
listed: false
---

## The Fundamental Theorem

Every integer greater than one has a unique factorization into primes. 60 = 2 x 2 x 3 x 5. No other decomposition exists. The primes are irreducible -- they cannot be broken further -- and the factorization is unique. Not a convention. A structural truth about the integers.

---

## Testing Composites

We did start testing the UI. Selenium, Protractor, Cypress, Playwright. Each one attempting the same fundamentally misguided operation: verifying a composite by testing the product directly.

The test for "user updates their email" is not testing one thing. It is testing the product of irreducible factors: authentication logic, form validation, state management, network communication, DOM rendering, CSS layout, animation timing. A composite with at least seven prime factors, some non-deterministic.

If you test that 2 x 2 x 3 x 5 = 60 and the test fails, which factor is wrong? You cannot tell from the product. Worse: if one factor is non-deterministic -- if the 3 is sometimes 3.01 because of animation timing -- the product fluctuates. The test passes sometimes and fails sometimes. Not because your primes are wrong, but because multiplying them introduces noise.

This is flakiness, understood mathematically. A flaky test is a test trying to verify primes by checking their non-deterministic product.

These tests also suffer from a curse of dimensionality. DOM, CSS, timing, network latency -- each an independent axis. The browser test occupies a point in the product space of all of them. In high-dimensional spaces, almost all the volume is at the boundary. You cannot sample densely enough. Dimensional reduction -- PCA dropping noise dimensions to reveal signal -- is the answer. Project onto the dimensions that carry information. Drop the rest.

---

## The Decomposition

Most E2E tests are testing business logic *through* the browser -- verifying primes by computing their product with rendering, timing, and DOM structure. Authentication, validation, state transition, persistence -- those are the prime factors. The browser is a composite multiplier that inflates the product space without adding information about the primes.

Cockburn's hexagonal architecture (2005) argued that applications should be equally drivable by users, programs, and test scripts. Fowler called it "subcutaneous testing." The factorization was always possible in theory. What was missing was a standard notation.

MCP -- Model Context Protocol -- is that notation. It exposes your application's capabilities as structured tools. Text in, text out. Each tool isolates a prime factor of behavior.

Think of it as a Fourier decomposition. Your application's behavior is a complex waveform -- a superposition of many frequencies. Business logic is the fundamental frequency: low, stable, carrying the essential information. UI rendering is high-frequency noise: fast-changing, environment-dependent, contributing energy but not meaning. Browser E2E tests sample the raw waveform and try to reason about the fundamental by staring at the noise. MCP is a low-pass filter that isolates the fundamental and discards the obscuring harmonics.

Here is a Cypress E2E test -- sampling the raw composite:

```typescript
// Cypress E2E test — testing the composite product
describe('Email update flow', () => {
  it('should allow user to change their email', () => {
    cy.login('test@example.com', 'password123');
    cy.visit('/settings');
    cy.get('[data-testid="email-input"]').clear().type('new@example.com');
    cy.get('[data-testid="save-button"]').click();
    cy.get('[data-testid="confirm-dialog"]').should('be.visible');
    cy.get('[data-testid="confirm-button"]').click();
    cy.get('[data-testid="success-toast"]').should('contain', 'Email updated');
    cy.visit('/profile');
    cy.get('[data-testid="user-email"]').should('contain', 'new@example.com');
  });
});
```

Every `cy.get` couples your verification to a non-essential dimension. Every `should('be.visible')` is a measurement in the DOM rendering axis that tells you nothing about authentication or persistence. You are testing 2 x 2 x 3 x 5 and hoping that if the product is 60, each factor must be correct. It is not a valid inference.

Now here is the same business logic factored into its primes:

```typescript
// MCP tool — isolating the prime factor
const updateEmailTool = {
  name: 'update_user_email',
  description: 'Update the authenticated user\'s email address',
  inputSchema: {
    type: 'object',
    properties: {
      newEmail: { type: 'string', format: 'email' },
      confirmChange: { type: 'boolean' },
    },
    required: ['newEmail', 'confirmChange'],
  },
  handler: async ({ newEmail, confirmChange }, context) => {
    const user = await context.getAuthenticatedUser();
    if (!user) return { error: 'Not authenticated' };

    if (!confirmChange) {
      return {
        status: 'confirmation_required',
        message: `Confirm email change from ${user.email} to ${newEmail}?`,
      };
    }

    await context.userService.updateEmail(user.id, newEmail);
    return {
      status: 'success',
      message: `Email updated to ${newEmail}`,
      updatedEmail: newEmail,
    };
  },
};

// Unit test — verifying the prime directly
describe('update_user_email', () => {
  it('should update email when confirmed', async () => {
    const context = createMockContext({
      user: { id: '1', email: 'old@example.com' },
    });

    const result = await updateEmailTool.handler(
      { newEmail: 'new@example.com', confirmChange: true },
      context,
    );

    expect(result.status).toBe('success');
    expect(result.updatedEmail).toBe('new@example.com');
    expect(context.userService.updateEmail).toHaveBeenCalledWith(
      '1', 'new@example.com',
    );
  });

  it('should require confirmation before updating', async () => {
    const context = createMockContext({
      user: { id: '1', email: 'old@example.com' },
    });

    const result = await updateEmailTool.handler(
      { newEmail: 'new@example.com', confirmChange: false },
      context,
    );

    expect(result.status).toBe('confirmation_required');
    expect(context.userService.updateEmail).not.toHaveBeenCalled();
  });
});
```

This test runs in milliseconds. It does not depend on DOM structure, animation timing, or CSS selectors. It verifies the prime factor -- authentication, confirmation, state change -- without multiplying it by dimensions that contribute nothing but noise.

You have not lost any coverage. You have factored out the noise.

---

## The Uniqueness Theorem

The power of the Fundamental Theorem is not just that every integer *has* a factorization. It is that the factorization is *unique*. 60 decomposes into primes exactly one way. This uniqueness makes primes a canonical basis for the integers.

MCP tools, when designed well, are the canonical basis for your application's behavior. `update_user_email` does one thing. It does not send notifications. It does not update the password. It is prime. The set of all your MCP tools is the unique factorization of your application's capability space.

A unit test and its code, when written properly, specify each other. Two representations of the same mathematical object.

MCP adds a third representation: the name and schema. That name is a constraint. The input schema is a constraint. The handler is a constraint. Each constrains the other two. In graph theory, this is a triangle where each edge carries enough information that, combined with domain rules, you can reconstruct the third vertex from any two.

Test. Code. Name. Three players, one truth. We had the first two for decades. MCP formalized the third into something machines can reason about -- and when you give the name enough structure to be machine-readable, the entire triangle becomes machine-verifiable.

---

## The Pyramid as Limit

The testing pyramid was always a local optimum imposed by computational constraints. We put E2E tests at the top because browser automation was the only way to verify the full composite. Browsers are slow. Therefore composite tests are slow. Therefore write fewer of them.

MCP removes the constraint. If your business logic is decomposed into prime factors accessible through a text interface, you do not need to compute the composite to verify the primes. Function calls are fast. Therefore prime-factor tests are fast. Therefore write as many as you want.

The pyramid does not flip. It undergoes dimensional reduction. The high-dimensional E2E layer -- DOM x CSS x timing x network x state -- projects down onto the essential business logic axes. What remains at the top is a thin layer of smoke tests that verify rendering and integration wiring. The primes move to the unit test layer where they belong.

Your CI pipeline benefits from the same decomposition. Run `yarn vitest run --changed main` -- only tests touching changed primes execute. An AI reviewer traces the dependency graph and determines which E2E composites a given PR actually affects. The test suite becomes a sparse graph rather than a dense matrix.

---

## Full Circle

The real answer to the UI testing question was never "automate the browser." The real answer was: decompose the composite into its prime factors and test each one directly.

Now we can. Not new tools. Better decompositions. The boring kind of structural insight that, like the Fundamental Theorem itself, seems obvious in hindsight and rearranges everything once you see it.
