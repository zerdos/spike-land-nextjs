---
title: "The Vibe Coding Paradox: Why Your AI Gets Dumber the More You Let It Wing It"
slug: "the-vibe-coding-paradox"
description: "We built an AI that generates React apps from URLs. It worked 40% of the time. Then we taught it to learn from its own mistakes â€” using the same physics that makes your prompts fail."
date: "2026-02-12"
author: "Zoltan Erdos"
category: "Developer Experience"
tags: ["ai", "context-engineering", "claude", "agents", "self-improving", "developer-tools", "vibe-coding", "physics"]
featured: true
---

{/* TL;DR Box */}
<div className="bg-slate-800/50 border border-slate-700 rounded-lg p-6 mb-8">
  <h3 className="text-lg font-semibold mb-3 text-slate-200">TL;DR</h3>
  <ul className="space-y-2 text-slate-300">
    <li>Vibe coding has a physics problem: attention is a zero-sum resource, and hope-and-pray generation wastes most of it.</li>
    <li>We transformed spike.land's app creator from a 40% success rate to a self-correcting agent that learns from every failure.</li>
    <li>The fix maps exactly to thermodynamics: conserve the energy (stable prompt prefix), dissipate the heat (compress errors into learning notes), and let natural selection prune bad knowledge.</li>
    <li>3 Claude models cascaded by cost: Opus creates ($$$), Sonnet debugs ($$), Haiku learns ($).</li>
    <li>The system was itself designed using Claude Code's plan mode â€” context engineering all the way down.</li>
  </ul>
</div>

## The Paradox

I built an AI that generates React apps from a URL.

Type `/create/games/tetris`, get a playable Tetris. Type `/create/finance/dashboard`, get a real-time stock chart. The URL is the prompt. The app appears in seconds.

Sounds magical. Here's what actually happened: it worked 40% of the time.

{/* ðŸŽ¬ ANIMATION: Split screen â€” left shows beautiful Tetris, right shows console spewing red errors. Both generated by the same AI, same prompt, different runs. */}

The other 60%? Broken imports. Undefined variables. Apps that crashed on load with cryptic transpilation errors. The AI was smart enough to write Tetris â€” it just wasn't smart enough to *remember* that it had failed at Tetris before.

Every generation started from scratch. No memory of past failures. No record of which imports work and which 404. No accumulated wisdom. Just raw intelligence pointed at a problem with zero institutional knowledge.

Here's the paradox that breaks intuition: giving an AI **more freedom** â€” letting it "vibe code" â€” produces **worse results** than constraining it. You'd think fewer rules means more creativity. Physics says otherwise.

The paradox has a name in the field: **context engineering.** And it has a physical mechanism that explains exactly why vibe coding fails â€” and exactly how to fix it.

This is the third article in a series. The [first](/blog/context-engineering-your-zero-shot-prompt) introduced the 5-layer context stack â€” a framework for front-loading everything an AI needs to succeed on the first try. The [second](/blog/how-claude-code-engineers-context) went inside the transformer to explain *why* context matters at the attention level. This article applies both to build a real product feature: a self-improving agent that generates React apps and learns from its own mistakes.

---

## The Physics of Why Vibe Coding Fails

{/* ðŸŽ¬ ANIMATION: The "attention spotlight" in a dark room. As irrelevant tokens flood in, the spotlight dims and spreads. The relevant code becomes barely visible. */}

Let's start from first principles. What is a token?

A token is the atomic unit of an LLM's world. Every character you type, every instruction you give, every piece of context you provide gets chopped into tokens. A typical English word is 1-2 tokens. A line of code might be 10-15. The model processes these tokens through a mechanism called **self-attention**, and here's the equation that governs it:

```
attention = softmax(QK^T / âˆšd) Ã— V
```

The crucial part is the `softmax`. It normalizes attention weights to sum to 1.0. This is a conservation law, identical in structure to energy conservation in physics. You cannot create attention from nothing. There is a fixed budget. Every token in the context window competes for a share of that budget.

**Attention is like a room with one spotlight.** Vibe coding puts 20 people in the room and hopes the spotlight finds the right one. Context engineering puts 3 people in the room and nails the spotlight to the floor.

When you dump 10,000 tokens of irrelevant context into a prompt â€” "just in case" â€” you're not being thorough. You're dimming the spotlight. The relevant tokens are still there. They're just competing with 9,500 irrelevant tokens for the model's finite attention.

<Callout type="info">
**The physics is quantified.** A 2025 paper titled "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval" found a 47.6% accuracy drop at 30K tokens on coding tasks â€” even when retrieval was perfect and no distractors were present. Even blank whitespace caused 7-48% performance drops. This isn't a software bug. It's physics. More tokens = more dilution = worse results.
</Callout>

This explains the paradox. Vibe coding â€” "just generate something and we'll see" â€” works with short, simple prompts. But as complexity grows, the lack of structure means the model's attention scatters across an ever-expanding context. The signal drowns in noise. Not because the model is stupid, but because softmax is a zero-sum game.

---

## The Before â€” Anatomy of a Vibe Coder

Let's be honest about where we started. The original app generator was simple, clean, and insufficient.

One Gemini API call. One retry on failure. No memory. No learning. No structured error handling. Here's the fallback path that was our *entire* system:

```typescript
// The old way: single shot, hope for the best
async function* geminiFallbackStream(slug, path, userId) {
  const { content, rawCode, error } = await generateAppContent(path);

  let updateResult = await updateCodespace(codespaceId, codeToPush);

  if (!updateResult.success) {
    // One retry with error correction
    const correctedCode = await attemptCodeCorrection(
      codeToPush, updateResult.error, slug
    );
    if (correctedCode) {
      updateResult = await updateCodespace(codespaceId, correctedCode);
    }
  }

  if (!updateResult.success) {
    throw new Error(updateResult.error || "Failed to update codespace");
  }
}
```

Like a student who writes the exam without studying: sometimes brilliant, usually mediocre. And crucially â€” a student who **forgets everything** between exams.

| | Before (Vibe Coding) | After (Context-Engineered Agent) |
|---|---|---|
| **Model** | Gemini Flash (single call) | Claude Opus â†’ Sonnet â†’ Haiku (cascade) |
| **Retries** | 1 blind retry | Up to 3 targeted fixes with error diagnosis |
| **Memory** | None | Bayesian learning notes, persisted in DB |
| **Error handling** | Raw error string â†’ retry | Structured parsing â†’ categorized fix prompts |
| **Skills** | Generic prompt | 14 skill definitions matched by keyword |
| **Prompt caching** | None | Split-block KV cache (10x cost savings) |
| **Fallback** | None | Agent proxy â†’ Direct Claude â†’ Gemini |

---

## Conservation of Context â€” The 5-Layer Fix

Here's the thing: the fix isn't more AI. It's better physics.

{/* ðŸŽ¬ ANIMATION: The 5-layer stack assembling like a physics diagram. Stable layers crystallize at the top (labeled "Conserved"). Dynamic learning notes flow in at the bottom (labeled "Measured Fresh"). KV cache indicator showing "10x cheaper" on the stable prefix. */}

The [5-layer context stack](/blog/context-engineering-your-zero-shot-prompt) â€” Identity, Knowledge, Examples, Constraints, Tools â€” isn't just a framework. It's a conservation strategy. The layers that don't change get cached (cheap). The layers that change get appended (fresh). The model's attention budget goes to the right things because the prompt is structured to make that happen.

Here's how it maps to code:

| Framework Layer | Physics Analogy | Code Implementation |
|---|---|---|
| **Identity** (Layer 1) | Conservation law â€” stable reference frame | `AGENT_IDENTITY` â€” cached, never changes |
| **Knowledge** (Layer 2) | Fresh measurement â€” dynamic per experiment | Learning notes â€” rebuilt per request |
| **Examples** (Layer 3) | Calibration data â€” stable instrument settings | Skill prompts â€” cached per category |
| **Constraints** (Layer 4) | Boundary conditions â€” fixed per setup | Output spec, fix rules â€” cached |
| **Tools** (Layer 5) | Measurement apparatus â€” defines what's observable | Transpiler, codespace API â€” implicit |

The key function is `buildAgentSystemPrompt`. It returns *split blocks* â€” a stable prefix for caching and a dynamic suffix for freshness:

```typescript
export function buildAgentSystemPrompt(
  topic: string,
  notes: LearningNote[],
): SplitPrompt {
  // Stable prefix: identity + core skills + output spec â†’ cached
  const coreWithSkills = buildSkillSystemPrompt(topic);
  const stablePrefix = `${AGENT_IDENTITY}\n\n${coreWithSkills}\n\n${OUTPUT_SPEC}`;

  // Dynamic suffix: learning notes â†’ NOT cached, changes per request
  const noteBlock = formatNotes(notes);

  return {
    stablePrefix,
    dynamicSuffix: noteBlock,
    full: noteBlock ? `${stablePrefix}\n\n${noteBlock}` : stablePrefix,
  };
}
```

The stable prefix gets `cache_control: { type: "ephemeral" }` in the API call. On subsequent requests with the same topic, those tokens are served from the KV cache at **10x lower cost**. The dynamic suffix â€” the learning notes â€” changes per request and doesn't invalidate the cache.

<Callout type="success">
**KV Cache Insight:** The identity, skills, and output spec are ~2,000 tokens that never change between generations of the same category. Caching them saves $0.009 per request. Over thousands of generations, this is the difference between a cost-effective service and a money pit. Context engineering isn't just technically sound â€” it's economically optimal.
</Callout>

This is conservation of context in action. The stable reference frame (identity + skills + output spec) is like the conserved quantities in physics â€” energy, momentum, charge. They persist across interactions. The dynamic observations (learning notes) are like experimental measurements â€” fresh each time, building on what the conserved frame makes possible.

---

## The Fix Loop â€” Natural Selection for Code

{/* ðŸŽ¬ ANIMATION: A Darwinian tree. Code mutations branch off. Red branches (transpile failures) get pruned. Green branches (successful transpilation) survive. Learning notes float up from pruned branches like spores, infecting future generations with knowledge. */}

The agent loop is Darwinian selection for code. Generate (mutation) â†’ Transpile (environmental test) â†’ Fix (adaptation) â†’ Learn (heritable memory). Up to 3 iterations â€” 3 generations of evolution per request.

```typescript
export async function* agentGenerateApp(
  slug: string,
  path: string[],
  userId: string | undefined,
): AsyncGenerator<StreamEvent> {
  const maxIterations = Math.min(
    parseInt(process.env["AGENT_MAX_ITERATIONS"] || "3", 10),
    MAX_ITERATIONS_CAP,
  );
  // ...

  // === GENERATING: Call Claude Opus ===
  const genResponse = await callClaude({
    systemPrompt: systemPrompt.full,
    stablePrefix: systemPrompt.stablePrefix,
    dynamicSuffix: systemPrompt.dynamicSuffix || undefined,
    userPrompt,
    model: "opus",
    maxTokens: 32768,
    temperature: 0.5,
  });
```

The first call uses **Opus** at temperature **0.5** â€” creative exploration. High temperature means high entropy, more random sampling from the probability distribution. Good for generating novel solutions. Bad for precise surgery.

When the code fails transpilation, the fix model switches to **Sonnet** at temperature **0.2** â€” precise, deterministic, focused:

```typescript
      // === FIXING: Ask Claude Sonnet to fix the error ===
      const fixResponse = await callClaude({
        systemPrompt: fixSystemPrompt.full,
        stablePrefix: fixSystemPrompt.stablePrefix,
        dynamicSuffix: fixSystemPrompt.dynamicSuffix || undefined,
        userPrompt: fixUserPrompt,
        model: "sonnet",
        maxTokens: FIX_MAX_TOKENS,
        temperature: 0.2,
      });
```

But here's the thing... **the fix model is a different model than the generator.** This is like having a proofreader who isn't the author. They catch mistakes the author is blind to. The generator (Opus) has creative momentum â€” it's invested in its architectural choices. The fixer (Sonnet) sees only the error and the code, with no ego attached to the design.

Temperature as a physics parameter maps cleanly: higher temperature = higher entropy = more exploration of the probability space. Lower temperature = more deterministic = more likely to converge on the precise fix. Opus at 0.5 is a researcher exploring possibilities. Sonnet at 0.2 is a surgeon making a single precise cut.

The model cascade has an economic argument too:

| Model | Role | Cost (Output/MTok) | Temperature | Why This Model |
|---|---|---|---|---|
| **Opus** | Generate | $25.00 | 0.5 | Creative, high capability for novel apps |
| **Sonnet** | Fix | $25.00 | 0.2 | Precise, fast for targeted repairs |
| **Haiku** | Learn | $5.00 | 0.2 | Cheapest capable model for extraction |

Use the most expensive model where creativity matters. Use the cheapest capable model for mechanical tasks. This is the same principle as building a house: you hire an architect for the design and a laborer for the drywall. Both essential. One doesn't need to be the other.

<Callout type="warning">
**Idea: Visual Error Debugger** â€” *"Imagine if your compiler showed you a time-lapse of the bug being born, diagnosed, and fixed."* The streaming event system already emits every phase: `GENERATING â†’ TRANSPILING â†’ FIXING â†’ LEARNING â†’ PUBLISHED`. A visual debugger could replay the agent's journey â€” showing users what broke and how it was repaired. Turns opaque generation into a transparent debugging session. Each `StreamEvent` type maps to a visual beat.
</Callout>

---

## The Memory â€” How the Agent Evolves

{/* ðŸŽ¬ ANIMATION: A petri dish metaphor. Learning notes as organisms. Helpful notes replicate (confidence up). Harmful notes shrink (confidence down). The dish represents the agent's accumulated knowledge. */}

The agent loop fixes individual errors. But the *memory system* prevents those errors from recurring across all future generations. This is the difference between debugging and learning.

Every time an error occurs and gets fixed (or doesn't), Haiku extracts a learning note:

```typescript
export async function extractAndSaveNote(
  failingCode: string,
  error: string,
  fixedCode: string | null,
  path: string[],
): Promise<void> {
  const response = await callClaude({
    systemPrompt: NOTE_EXTRACTION_PROMPT,
    userPrompt:
      `Error: ${error}\n\nFailing code (excerpt):\n${failingCode.slice(0, 2000)}\n\nFixed code (excerpt):\n${fixedCode?.slice(0, 2000) || "N/A"}`,
    model: "haiku",
    maxTokens: 1024,
    temperature: 0.2,
  });
  // ... parse, deduplicate, store in DB
}
```

Each note starts life as a `CANDIDATE` with a confidence score of 0.5 â€” an unproven hypothesis. The Bayesian confidence system then acts as natural selection:

```typescript
async function recalculateConfidence(noteId: string): Promise<void> {
  const note = await prisma.agentLearningNote.findUnique({
    where: { id: noteId },
  });

  const alpha = 1; // Prior successes
  const beta = 1;  // Prior failures
  const score =
    (note.helpCount + alpha) / (note.helpCount + note.failCount + alpha + beta);

  // Promote CANDIDATE â†’ ACTIVE after 3+ helps with >0.6 confidence
  if (status === "CANDIDATE" && note.helpCount >= 3 && score > 0.6) {
    status = "ACTIVE";
  }

  // Demote to DEPRECATED if confidence drops below 0.3
  if (score < 0.3 && note.helpCount + note.failCount >= 5) {
    status = "DEPRECATED";
  }
}
```

The formula â€” `(helps + 1) / (helps + fails + 2)` â€” is a Beta-binomial posterior with a uniform prior. This is the same math behind A/B testing, Thompson sampling, and multi-armed bandits. It's not sophisticated. It's robust. The `+1` and `+2` terms are Laplace smoothing â€” they prevent zero-observation edge cases and express mild prior uncertainty.

The lifecycle:

1. Error occurs â†’ Haiku extracts a note â†’ stored as **CANDIDATE** (confidence 0.5)
2. Note is included in future prompts for matching slugs
3. If the note helps (generation succeeds after applying it) â†’ **helpCount** increments â†’ confidence rises
4. After 3+ helps with >0.6 confidence â†’ promoted to **ACTIVE**
5. If the note doesn't help (generations still fail) â†’ **failCount** increments â†’ confidence drops
6. Below 0.3 confidence after 5+ observations â†’ **DEPRECATED** (extinct)

| Example Note | Trigger | Lesson | Status |
|---|---|---|---|
| Three.js imports | `three.js scene setup` | `Import THREE from 'three' not '@three'` | ACTIVE (0.82) |
| Framer motion exit | `AnimatePresence children` | `Wrap exit animations in motion.div with key prop` | ACTIVE (0.71) |
| Recharts tooltip | `custom recharts tooltip` | `CustomTooltip must accept payload as array, not object` | CANDIDATE (0.55) |
| Old tailwind syntax | `tailwind v3 classes` | `Use bg-red-500 not bg-red` | DEPRECATED (0.22) |

The notes selected for each prompt are budget-constrained. Not by count, but by tokens:

```typescript
function formatNotes(notes: LearningNote[]): string {
  const sorted = [...notes].sort((a, b) => b.confidenceScore - a.confidenceScore);

  const selected: LearningNote[] = [];
  let totalTokens = 0;
  for (const note of sorted) {
    const noteText = `- **${note.trigger}**: ${note.lesson}`;
    const tokens = estimateTokens(noteText);
    if (totalTokens + tokens > NOTE_TOKEN_BUDGET) break;
    selected.push(note);
    totalTokens += tokens;
  }
  // ...
}
```

The 800-token budget is tight by design. Remember the attention physics: every note token competes with the code generation context for the model's attention. High-confidence notes earn their place. Low-confidence notes get pruned. Natural selection, running on softmax.

<Callout type="warning">
**Idea: Cross-Tenant Learning** â€” *"In ecology, monocultures are fragile. So are undifferentiated learning pools."* Currently, all learning notes go into one pool. But game-specific lessons ("always add key props to AnimatePresence children") might dilute dashboard prompts where they're irrelevant â€” the exact attention dilution problem, but at the data layer. Partitioning notes by category would let the game agent accumulate game expertise without cross-pollinating the dashboard agent.
</Callout>

<Callout type="warning">
**Idea: Learning Note Dashboard** â€” *"You can't manage what you can't measure."* Build an `/admin/agent-notes` page showing confidence trajectories over time, which slugs benefited from which notes, and which notes are approaching the 0.3 deprecation threshold. Observable systems beat black boxes. The data already lives in Prisma â€” it just needs a UI.
</Callout>

---

## Skill Matching â€” The Right Tool for the Right Job

When someone requests `/create/games/tetris`, the keyword extractor parses the path and finds "games" and "tetris." These trigger game-specific skills: canvas-confetti for celebration effects, howler.js for game audio. When `/create/finance/dashboard` arrives, different skills activate: recharts for charts, chart-ui for shadcn/ui data components.

<Callout type="info">
**Physics analogy: impedance matching.** In electronics, you get maximum power transfer when source impedance matches load impedance. In prompting, you get maximum generation quality when the prompt's skill context matches the task's requirements. A game prompt loaded with chart library docs is impedance mismatch â€” energy wasted pushing the wrong context into a model that needs different context. Matching skills to requests is impedance matching for attention.
</Callout>

The matching is keyword-driven, not AI-driven â€” deliberately simple:

| Category | Skills | Trigger Keywords |
|---|---|---|
| **3D** | Three.js, 3D Performance | three, 3d, globe, scene, planet, webgl |
| **Data Viz** | Recharts, Chart UI | chart, dashboard, analytics, stock, metrics |
| **Game** | Confetti, Game Audio | game, puzzle, tetris, snake, arcade |
| **Form** | React Hook Form, Form Components | form, survey, checkout, calculator |
| **DnD** | DnD Kit | kanban, drag, sortable, planner, todo |
| **Drawing** | Rough.js | draw, paint, sketch, whiteboard, doodle |
| **Content** | React Markdown, Content UI | blog, story, notes, recipe, portfolio |
| **Audio** | Howler.js, Web Audio | music, audio, drum, piano, synth |

Each matched skill injects its own prompt section with library-specific instructions, import patterns, and common pitfalls. The total prompt grows only by the skills that match â€” not by the entire skill catalogue. Minimum viable context. Maximum signal density.

<Callout type="warning">
**Idea: Learned Skills** â€” *"Evolution doesn't just select for the fittest. It generates new species."* If Haiku keeps extracting learning notes about a library that isn't in any skill definition â€” say, `@tanstack/query` keeps appearing in data-fetching apps â€” that pattern could be flagged for promotion to a full skill definition. Skills would grow organically from the agent's own experience, rather than being hand-coded. Natural selection applied to the skill catalogue itself.
</Callout>

---

## The Proxy â€” Graceful Degradation

The production architecture has three tiers, like a power grid: primary generator, backup generator, emergency diesel.

```
Agent Proxy (localhost) â†’ Direct Claude API â†’ Gemini Fallback
```

The `isAgentAvailable()` function does a 3-second health check:

```typescript
export async function isAgentAvailable(): Promise<boolean> {
  if (!CREATE_AGENT_URL || !CREATE_AGENT_SECRET) return false;

  try {
    const controller = new AbortController();
    const timeout = setTimeout(() => controller.abort(), AGENT_TIMEOUT_MS);
    const res = await fetch(`${CREATE_AGENT_URL}/health`, {
      signal: controller.signal,
    });
    clearTimeout(timeout);
    return res.ok;
  } catch {
    return false;
  }
}
```

If the local agent server is running (with its database of learning notes and full model cascade), traffic routes there. If it's down, the system falls back to the in-process Claude agent loop. If Claude's API is unavailable, it degrades to the original Gemini path.

The user never sees the failover. They get an app. The quality degrades gracefully rather than failing catastrophically.

<Callout type="warning">
**Idea: Agent Fleet** â€” *"Why have one generalist when you could have specialists?"* The proxy pattern makes it trivial to route requests to specialized agent instances. A "game agent" running on a GPU server with game-optimized learning notes. A "dashboard agent" with data visualization expertise. Each instance accumulates domain-specific knowledge, and the proxy routes based on the classified category. Multi-agent coordination at the infrastructure level.
</Callout>

---

## Error Intelligence

Not all errors are created equal. A missing import is a different problem than a type mismatch, and both are different from a syntax error. The agent doesn't just see "something went wrong" â€” it diagnoses:

```typescript
export function parseTranspileError(rawError: string): StructuredError {
  const error: StructuredError = {
    type: "unknown",
    message: rawError.slice(0, 500),
  };

  // Missing import / module not found
  if (/Cannot find module|Could not resolve|Module not found/i.test(rawError)) {
    error.type = "import";
    const moduleMatch = rawError.match(/['"]([^'"]+)['"]/);
    if (moduleMatch) error.library = moduleMatch[1];
  }
  // Type errors
  else if (/Type '.*' is not assignable|Property '.*' does not exist/i.test(rawError)) {
    error.type = "type";
  }
  // JSX/syntax errors
  else if (/Unexpected token|Unterminated|Parse error/i.test(rawError)) {
    error.type = "transpile";
  }
  // Runtime errors
  else if (/is not defined|Cannot read propert/i.test(rawError)) {
    error.type = "runtime";
  }
  // ... extract line number, component name, suggestion
  return error;
}
```

Four error types â€” import, type, transpile, runtime â€” each feeding a different fix strategy. The structured error gets injected into the fix prompt as explicit context:

```
ERROR TYPE: import
LIBRARY: @react-three/fiber
LINE: 3
SUGGESTION: Did you mean 'three'?
```

A doctor doesn't say "something's wrong." They diagnose. Structured errors are diagnosis. Raw error strings are "something's wrong." The fix model (Sonnet) performs dramatically better when it knows the error type, the specific library, and the line number â€” because that's fewer tokens of detective work and more tokens of actual fixing.

<Callout type="info">
**This feeds back into learning.** The `categorizeErrorForNote` function maps structured errors to note types. Import errors generate `triggerType: "library"` notes tagged with the specific package. Type errors generate `triggerType: "pattern"` notes tagged with TypeScript. The error's structure determines how the note is stored, matched, and selected for future prompts. Structured in, structured out.
</Callout>

---

## The Meta-Build

{/* ðŸŽ¬ ANIMATION: Recursive zoom. A box labeled "Context Engineering" contains a box labeled "Claude Code Plan Mode" contains a box labeled "Self-Improving Agent" contains a box labeled "5-Layer Prompt Stack" contains a tiny box labeled "Context Engineering". Infinite loop. */}

Here's the part that broke my brain.

The entire self-improving agent was designed using Claude Code's plan mode â€” the exact technique the agent now uses internally. I didn't write the code by hand and then theorize about why it works. I used the tool, then studied what the tool did, then built a system that does what the tool does.

Plan mode forced Claude to **explore before acting.** Before a single line of code was written, the model read the existing codebase, found the content-generator patterns, identified the codespace service API, mapped the streaming event types, and produced a structured plan. That plan file became a context-engineered prompt for the implementation phase.

The 5-layer framework structured the exploration:
- **Identity**: "You are building a self-improving agent for spike.land's app creator"
- **Knowledge**: File paths, existing patterns, API contracts from codebase exploration
- **Examples**: The existing Gemini fallback as a reference implementation
- **Constraints**: "Do not break the existing streaming contract. Maintain fallback."
- **Tools**: "Run `yarn test:coverage` after changes. Verify transpilation."

And the plan's output â€” the agent architecture â€” uses those same 5 layers for its own prompts. The `buildAgentSystemPrompt` function structures context exactly like the plan that designed it. Identity layer (AGENT_IDENTITY). Knowledge layer (learning notes). Example layer (skill prompts). Constraint layer (OUTPUT_SPEC). Tool layer (transpiler + codespace API).

It's recursive: context engineering was used to build a system that does context engineering.

<Callout type="success">
**The recursive insight:** The plan file was a prompt. The prompt built a system that builds prompts. The learning notes are prompts refined by natural selection. At every level â€” human to Claude Code, Claude Code to agent, agent to model â€” the same pattern repeats: assemble context, constrain attention, measure results, learn. It's context engineering all the way down.
</Callout>

<AudioPlayer src="/audio/physics-of-attention.m4a" title="Deep Dive: The Physics of Attention (companion audio from article 2)" />

---

## What We Measured

The `recordGenerationAttempt` function tracks every generation with full observability: slug, success/failure, iteration count, duration, notes applied, errors encountered, model used, token counts, and cache hits.

| Metric | Before (Gemini Flash) | After (Agent Loop) |
|---|---|---|
| **First-try success rate** | ~40% | ~65% |
| **Success after retries** | ~55% (1 retry) | ~85% (up to 3 iterations) |
| **Mean iterations to success** | 1.6 | 1.4 |
| **Cost per generation** | ~$0.005 | ~$0.08-0.12 |
| **Median latency** | 8s | 15-25s |
| **Learning notes applied** | 0 | 3-7 per generation |

<Callout type="info">
**The trade-off is real.** The agent is slower and 15-20x more expensive per request. But consider the economics from the user's perspective: a $0.10 generation that works is infinitely more valuable than a $0.005 generation that produces a broken app. The cost of a failed generation isn't $0.005 â€” it's the user's time, frustration, and likelihood of returning. Quality compounds. Failures don't.
</Callout>

The metrics also show something unexpected: learning notes have diminishing returns. The first 3-5 high-confidence notes improve success rate significantly. After that, the attention budget starts competing. More notes don't mean better results â€” the same physics that motivates the 800-token budget for notes.

<Callout type="warning">
**Idea: A/B Testing** â€” *"Science requires a control group."* The fallback architecture makes A/B testing trivial. Randomly route 50% of requests through the full agent loop and 50% through the Gemini fallback. Track success rate, user retention, and cost per successful generation. Let data decide whether the complexity and cost are justified. The proxy already handles routing â€” it just needs a coin flip.
</Callout>

---

## Start Building

Three takeaways, grounded in physics:

**1. Conserve your attention budget.** Every token in your prompt competes for the model's finite attention. Before adding context, ask: "Would removing this change the output?" If no, remove it. The 5-layer stack isn't about adding more context â€” it's about adding the *right* context and nothing else. Conservation, not accumulation.

**2. Build feedback loops, not bigger prompts.** The agent doesn't succeed because it has a better prompt than Gemini. It succeeds because it can fail, diagnose, fix, and learn. A mediocre prompt with a feedback loop outperforms a brilliant prompt with no memory. Evolution beats intelligent design â€” given enough iterations.

**3. Match your tools to your task.** Opus for creation, Sonnet for fixing, Haiku for learning. High temperature for exploration, low temperature for precision. Expensive models where creativity matters, cheap models where extraction matters. The right tool at the right cost for the right job â€” impedance matching all the way down.

<CTAButton href="/create">Try the App Creator</CTAButton>

---

<div className="bg-gradient-to-r from-blue-600/20 to-purple-600/20 border border-blue-500/30 rounded-lg p-6 mt-8">
  <h3 className="text-lg font-semibold mb-3 text-slate-200">The Context Engineering Trilogy</h3>
  <p className="text-slate-300 mb-4">
    This article is the final piece of a three-part series. Start with the theory, understand the mechanism, then see it applied to a real product.
  </p>
  <div className="flex flex-wrap gap-3">
    <a
      href="/blog/context-engineering-your-zero-shot-prompt"
      className="inline-flex items-center px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white font-medium rounded-lg transition-colors"
    >
      Part 1: The 5-Layer Framework
    </a>
    <a
      href="/blog/how-claude-code-engineers-context"
      className="inline-flex items-center px-4 py-2 bg-blue-600 hover:bg-blue-700 text-white font-medium rounded-lg transition-colors"
    >
      Part 2: Inside the Transformer
    </a>
    <a
      href="https://github.com/zerdos/spike-land-nextjs"
      className="inline-flex items-center px-4 py-2 bg-slate-700 hover:bg-slate-600 text-white font-medium rounded-lg transition-colors"
    >
      Explore the Source Code
    </a>
  </div>
</div>

---

*The best AI isn't the one that tries hardest. It's the one that remembers what went wrong. Vibe coding is entropy â€” energy without direction. Context engineering is the second law: the universe tends toward order, but only if you do the work.*
