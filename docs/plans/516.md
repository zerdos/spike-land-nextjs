# Implementation Plan: Hypothesis AI Agent - Experimentation Framework

**Issue**: #516
**Epic**: [Epic] Hypothesis AI Agent - Experimentation Framework
**Type**: Epic (Phase 3 - Autonomy Layer)
**Priority**: P2
**Labels**: phase:3-autonomy, layer:3-ai-agents, type:epic, orbit

---

## Executive Summary

This epic introduces an AI-powered experimentation framework that generates hypotheses, creates A/B tests, and provides statistical insights for content and campaign optimization. The Hypothesis AI Agent leverages existing A/B testing infrastructure and extends it with autonomous hypothesis generation, experiment design, and result interpretation.

**Dependencies**: Epic 3.3 (Orchestration) - satisfied by existing workflow executor system

---

## Stories Breakdown

### ORB-046: Create A/B Test Framework

**Status**: Foundation exists, needs AI agent integration
**Complexity**: Medium

Extends existing A/B testing infrastructure (`/src/lib/ab-testing.ts`, `SocialPostAbTest` models) with AI-powered hypothesis generation and automated experiment creation.

### ORB-047: Implement Content Experiments

**Status**: Not started
**Complexity**: Medium

Integrates hypothesis agent with content publishing system to deliver experiment variants and track outcomes.

---

## Acceptance Criteria Mapping

| Criterion                                   | Implementation Strategy                                                 | Status                       |
| ------------------------------------------- | ----------------------------------------------------------------------- | ---------------------------- |
| Define experiment with control/variant      | Extend `SocialPostAbTest` model + UI components                         | ✅ Foundation exists         |
| Statistical significance calculation        | Use existing `calculateChiSquared()` and `isStatisticallySignificant()` | ✅ Complete                  |
| Auto-winner selection option                | Extend existing `getWinner()` with auto-promotion workflow              | ⚠️ Needs workflow integration |
| Results dashboard with confidence intervals | Build on `/src/components/orbit/ab-tests/`                              | ⚠️ Needs enhancement          |

---

## Technical Architecture

### 1. Core Components

```
Hypothesis AI Agent Pipeline
├─ 1. Hypothesis Generation (AI)
│  ├─ Analyze historical data (analytics, campaign metrics)
│  ├─ Generate hypotheses using Claude
│  └─ Rank by expected impact + feasibility
│
├─ 2. Experiment Design (Orchestration)
│  ├─ Create variant specifications
│  ├─ Calculate required sample size
│  └─ Configure statistical parameters (α, power)
│
├─ 3. Variant Creation (Content)
│  ├─ AI-generate variant content
│  ├─ Apply Brand Brain guardrails
│  └─ Create SocialPostAbTestVariant records
│
├─ 4. Execution (Publishing)
│  ├─ Schedule variant publishing
│  ├─ Track impressions/engagements
│  └─ Monitor statistical significance
│
└─ 5. Analysis (Insights)
   ├─ Calculate confidence intervals
   ├─ Determine winner
   └─ Generate insights report
```

### 2. Data Models (Schema Extensions)

**New Models** (add to `prisma/schema.prisma`):

```prisma
/// AI-generated hypotheses for experiments
model Hypothesis {
  id               String   @id @default(cuid())
  workspaceId      String

  // Hypothesis details
  title            String
  description      String   @db.Text
  theoreticalBasis String?  @db.Text // Why this might work
  expectedOutcome  String?  @db.Text // Predicted result

  // AI metadata
  confidence       Float?   // AI's confidence (0-1)
  generatedBy      String   @default("hypothesis-agent")
  reasoning        String?  @db.Text // AI's reasoning

  // Experiment linkage
  experimentId     String?  @unique // Link to SocialPostAbTest

  // Status tracking
  status           HypothesisStatus @default(PROPOSED)
  priority         Int      @default(0) // 0-100 ranking

  // Audit
  createdAt        DateTime @default(now())
  updatedAt        DateTime @updatedAt

  workspace        Workspace @relation(fields: [workspaceId], references: [id], onDelete: Cascade)
  experiment       SocialPostAbTest? @relation(fields: [experimentId], references: [id])

  @@index([workspaceId])
  @@index([status])
  @@index([priority])
}

enum HypothesisStatus {
  PROPOSED      // AI generated, awaiting review
  APPROVED      // User approved, ready for experiment
  TESTING       // Experiment running
  VALIDATED     // Hypothesis confirmed by experiment
  REJECTED      // Hypothesis disproven
  ARCHIVED      // Not tested or outdated
}

/// Detailed experiment results with statistical analysis
model ExperimentResult {
  id               String   @id @default(cuid())
  experimentId     String   // SocialPostAbTest ID
  variantId        String?  // SocialPostAbTestVariant ID

  // Metric tracking
  metricName       String   // "engagement_rate", "ctr", "conversion_rate"
  metricValue      Float
  sampleSize       Int

  // Statistical analysis
  confidenceLevel  Float?   // 0.95 for 95% CI
  confidenceInterval Json?  // {lower: X, upper: Y}
  pValue           Float?
  effect           String?  // "POSITIVE", "NEGATIVE", "NEUTRAL"
  effectSize       Float?   // Cohen's d or similar

  // AI insights
  interpretation   String?  @db.Text // AI-generated explanation

  // Audit
  createdAt        DateTime @default(now())

  experiment       SocialPostAbTest @relation(fields: [experimentId], references: [id], onDelete: Cascade)
  variant          SocialPostAbTestVariant? @relation(fields: [variantId], references: [id])

  @@index([experimentId])
  @@index([variantId])
  @@index([metricName])
}

/// Extend SocialPostAbTest with hypothesis link
model SocialPostAbTest {
  // ... existing fields ...

  hypothesis       Hypothesis?
  results          ExperimentResult[]
}

model SocialPostAbTestVariant {
  // ... existing fields ...

  results          ExperimentResult[]
}
```

**Schema Migration Strategy**:

1. Create migration: `prisma migrate dev --name add-hypothesis-agent-models`
2. Seed with sample hypotheses for testing
3. Backfill existing experiments with "auto-generated" hypotheses

### 3. Agent Implementation

**New File**: `/src/lib/agents/hypothesis-agent.ts` (~400 lines)

```typescript
import { createClaudePrompt } from "@/lib/ai/prompts";
import { getWorkspaceAnalytics } from "@/lib/analytics/analytics-service";
import { getWorkspaceBrandProfile } from "@/lib/brand/profile-service";
import { VercelSandboxAgent } from "@/lib/sandbox/agent-sandbox";

export class HypothesisAgent {
  /**
   * Generate hypotheses based on workspace data
   *
   * Analyzes:
   * - Historical campaign performance
   * - Content engagement patterns
   * - Brand voice and guardrails
   * - Competitive benchmarks (if available)
   */
  async generateHypotheses(params: {
    workspaceId: string;
    count?: number; // How many hypotheses to generate
    focus?: "engagement" | "conversions" | "reach"; // Optimization goal
  }): Promise<Hypothesis[]>;

  /**
   * Design experiment for a hypothesis
   *
   * Creates:
   * - Variant specifications
   * - Sample size calculation
   * - Success metrics definition
   */
  async designExperiment(params: {
    hypothesisId: string;
    variants: number; // How many variants (2-4)
    primaryMetric: string;
    secondaryMetrics?: string[];
  }): Promise<ExperimentDesign>;

  /**
   * Generate variant content
   *
   * Uses:
   * - Brand Brain for voice/guardrails
   * - Original content as control
   * - Hypothesis for variation direction
   */
  async generateVariants(params: {
    hypothesisId: string;
    originalContent: string;
    count: number;
  }): Promise<VariantContent[]>;

  /**
   * Analyze experiment results
   *
   * Provides:
   * - Statistical significance
   * - Winner declaration
   * - Confidence intervals
   * - AI-generated insights
   */
  async analyzeResults(params: {
    experimentId: string;
  }): Promise<ExperimentAnalysis>;

  /**
   * Auto-select winner and promote
   *
   * Workflow:
   * 1. Check statistical significance
   * 2. Select winner variant
   * 3. Update experiment status
   * 4. Optional: Apply learnings to future content
   */
  async selectWinner(params: {
    experimentId: string;
    autoPromote?: boolean;
  }): Promise<WinnerSelection>;
}
```

**Implementation Pattern**:

- Extends `VercelSandboxAgent` for long-running AI tasks
- Uses workflow executor for multi-step orchestration
- Integrates with existing `ab-testing.ts` statistical library
- Stores results in `Hypothesis` and `ExperimentResult` models

### 4. Workflow Integration

**New File**: `/src/lib/workflows/actions/hypothesis-agent-actions.ts` (~200 lines)

Step handlers for workflow integration:

```typescript
// Register custom step handlers
registerStepHandler("generate_hypothesis", async (step, context) => {
  const agent = new HypothesisAgent();
  const hypotheses = await agent.generateHypotheses({
    workspaceId: context.workspaceId,
    count: step.config.count || 3,
    focus: step.config.focus || "engagement",
  });

  return { output: { hypotheses } };
});

registerStepHandler("design_experiment", async (step, context) => {
  const agent = new HypothesisAgent();
  const design = await agent.designExperiment({
    hypothesisId: step.config.hypothesisId,
    variants: step.config.variants || 2,
    primaryMetric: step.config.primaryMetric,
  });

  return { output: { design } };
});

registerStepHandler("analyze_experiment", async (step, context) => {
  const agent = new HypothesisAgent();
  const analysis = await agent.analyzeResults({
    experimentId: step.config.experimentId,
  });

  // Auto-select winner if configured
  if (step.config.autoSelectWinner && analysis.isSignificant) {
    await agent.selectWinner({
      experimentId: step.config.experimentId,
      autoPromote: step.config.autoPromote,
    });
  }

  return { output: { analysis } };
});
```

**Workflow DSL Example** (store in `WorkflowVersion`):

```yaml
name: "Content Optimization Experiment"
triggers:
  - type: SCHEDULED
    schedule: "0 9 * * MON" # Every Monday at 9am

steps:
  - id: generate_hypotheses
    type: ACTION
    handler: generate_hypothesis
    config:
      count: 5
      focus: engagement

  - id: select_top_hypothesis
    type: CONDITION
    condition: "{{generate_hypotheses.hypotheses.length}} > 0"
    branches:
      - type: IF_TRUE
        nextStepId: design_experiment
      - type: IF_FALSE
        nextStepId: end

  - id: design_experiment
    type: ACTION
    handler: design_experiment
    config:
      hypothesisId: "{{generate_hypotheses.hypotheses[0].id}}"
      variants: 3
      primaryMetric: engagement_rate

  - id: create_variants
    type: ACTION
    handler: generate_variants
    config:
      hypothesisId: "{{design_experiment.hypothesisId}}"
      count: 3

  - id: schedule_publishing
    type: ACTION
    handler: schedule_post_variants
    config:
      variants: "{{create_variants.variants}}"
      publishAt: "{{design_experiment.startDate}}"
```

### 5. API Routes

**New Routes** (add to `/src/app/api/orbit/[workspaceSlug]/experiments/`):

```
POST   /api/orbit/{workspace}/experiments/hypotheses/generate
  → Generate hypotheses using AI
  → Body: { count?: number, focus?: string }
  → Returns: Hypothesis[]

GET    /api/orbit/{workspace}/experiments/hypotheses
  → List all hypotheses
  → Query: ?status=PROPOSED&limit=10
  → Returns: { hypotheses: Hypothesis[], total: number }

PUT    /api/orbit/{workspace}/experiments/hypotheses/{id}/approve
  → Approve hypothesis and create experiment
  → Body: { variants: number, primaryMetric: string }
  → Returns: { hypothesis: Hypothesis, experiment: SocialPostAbTest }

POST   /api/orbit/{workspace}/experiments/{id}/analyze
  → Trigger analysis of experiment results
  → Returns: ExperimentAnalysis

POST   /api/orbit/{workspace}/experiments/{id}/select-winner
  → Auto-select winner based on statistical significance
  → Body: { autoPromote?: boolean }
  → Returns: WinnerSelection

GET    /api/orbit/{workspace}/experiments/{id}/results
  → Get detailed experiment results with confidence intervals
  → Returns: ExperimentResult[]
```

**Implementation Files**:

- `/src/app/api/orbit/[workspaceSlug]/experiments/hypotheses/generate/route.ts` (~100 lines)
- `/src/app/api/orbit/[workspaceSlug]/experiments/hypotheses/route.ts` (~80 lines)
- `/src/app/api/orbit/[workspaceSlug]/experiments/hypotheses/[id]/approve/route.ts` (~120 lines)
- `/src/app/api/orbit/[workspaceSlug]/experiments/[id]/analyze/route.ts` (~150 lines)
- `/src/app/api/orbit/[workspaceSlug]/experiments/[id]/select-winner/route.ts` (~100 lines)
- `/src/app/api/orbit/[workspaceSlug]/experiments/[id]/results/route.ts` (~90 lines)

### 6. UI Components

**New Components** (add to `/src/components/orbit/experiments/`):

```
/src/components/orbit/experiments/
├── hypothesis-list.tsx              # List hypotheses with status badges
├── hypothesis-card.tsx              # Single hypothesis display
├── hypothesis-generator.tsx         # Generate new hypotheses (AI button)
├── experiment-designer.tsx          # Configure experiment parameters
├── variant-preview.tsx              # Preview variants side-by-side
├── results-dashboard.tsx            # Statistical results visualization
│   ├── confidence-interval-chart.tsx
│   ├── significance-badge.tsx
│   └── winner-announcement.tsx
└── auto-winner-toggle.tsx           # Enable/disable auto-selection
```

**Page Routes** (add to `/src/app/orbit/[workspaceSlug]/experiments/`):

```
/orbit/{workspace}/experiments
  → Main experiments dashboard
  → List active experiments + hypotheses
  → "Generate Hypotheses" CTA

/orbit/{workspace}/experiments/hypotheses
  → Hypothesis backlog
  → Filter by status, priority
  → Approve/reject actions

/orbit/{workspace}/experiments/{id}
  → Experiment detail view
  → Variant comparison
  → Live results with confidence intervals
  → Winner selection UI
```

**Implementation Files**:

- `/src/app/orbit/[workspaceSlug]/experiments/page.tsx` (~250 lines) - Dashboard
- `/src/app/orbit/[workspaceSlug]/experiments/hypotheses/page.tsx` (~200 lines) - Hypothesis list
- `/src/app/orbit/[workspaceSlug]/experiments/[id]/page.tsx` (~300 lines) - Experiment detail

### 7. Statistical Analysis Enhancements

**Extend**: `/src/lib/ab-testing.ts` (~50 new lines)

Add:

```typescript
/**
 * Calculate confidence intervals for variant metrics
 */
export function calculateConfidenceInterval(
  successes: number,
  total: number,
  confidenceLevel: number = 0.95,
): { lower: number; upper: number; } {
  // Wilson score interval (more accurate for proportions)
  const z = getZScore(confidenceLevel);
  const p = successes / total;
  const denominator = 1 + (z * z) / total;

  const center = (p + (z * z) / (2 * total)) / denominator;
  const margin = (z * Math.sqrt((p * (1 - p) + (z * z) / (4 * total)) / total)) / denominator;

  return {
    lower: center - margin,
    upper: center + margin,
  };
}

/**
 * Calculate effect size (Cohen's h for proportions)
 */
export function calculateEffectSize(
  p1: number, // Control proportion
  p2: number, // Variant proportion
): number {
  const phi1 = 2 * Math.asin(Math.sqrt(p1));
  const phi2 = 2 * Math.asin(Math.sqrt(p2));
  return phi2 - phi1;
}

/**
 * Interpret effect size
 */
export function interpretEffectSize(h: number): "SMALL" | "MEDIUM" | "LARGE" {
  const absH = Math.abs(h);
  if (absH < 0.2) return "SMALL";
  if (absH < 0.5) return "MEDIUM";
  return "LARGE";
}
```

---

## Integration Points

### 1. With Existing A/B Testing Infrastructure

**Files to Modify**:

- `/src/lib/ab-testing.ts` - Add confidence interval and effect size functions
- `/src/app/api/orbit/[workspaceSlug]/ab-tests/route.ts` - Link hypotheses to tests
- `/src/components/orbit/ab-tests/ab-test-card.tsx` - Show hypothesis if linked

**Integration Strategy**:

- `SocialPostAbTest` gets optional `hypothesisId` field
- When creating A/B test from hypothesis, link via foreign key
- Display hypothesis reasoning in A/B test UI
- Use hypothesis confidence to prioritize experiments

### 2. With Publishing System

**Files to Modify**:

- `/src/lib/calendar/publishing-service.ts` - Support experiment variant publishing
- `/src/app/api/cron/publish-scheduled-posts/route.ts` - Track experiment impressions

**Integration Strategy**:

- Variant content stored in `SocialPostAbTestVariant.content`
- Publishing service selects variant based on experiment assignment
- Track impressions/engagements in existing analytics models
- Use `CampaignAttribution` for conversion tracking

### 3. With Analytics & Tracking

**Files to Modify**:

- `/src/lib/analytics/analytics-service.ts` - Add experiment result aggregation
- `/src/app/api/tracking/event/route.ts` - Tag events with variant ID

**Integration Strategy**:

- Custom analytics events for experiment interactions
- Segment data by variant ID
- Aggregate results into `ExperimentResult` model
- Calculate metrics: engagement_rate, ctr, conversion_rate

### 4. With Brand Brain

**Files to Modify**:

- `/src/lib/brand/profile-service.ts` - Export brand context for hypothesis generation
- `/src/lib/agents/hypothesis-agent.ts` - Use brand profile in variant generation

**Integration Strategy**:

- Hypothesis agent reads `BrandProfile` for tone and guardrails
- Variant generation respects `BrandVocabulary` and `BrandGuardrail`
- AI prompts include brand mission and tone descriptors
- Guardrail violations prevent variant creation

### 5. With Workflow Orchestration

**Files to Create/Modify**:

- `/src/lib/workflows/actions/hypothesis-agent-actions.ts` - New step handlers
- `/src/lib/workflows/workflow-executor.ts` - No changes (already extensible)

**Integration Strategy**:

- Register custom step handlers for hypothesis operations
- Use workflow DSL to define experiment lifecycle
- Trigger workflows on events (e.g., `POST_PUBLISHED`, `ENGAGEMENT_THRESHOLD`)
- Store workflow definitions in `WorkflowVersion` model

---

## Testing Strategy

### Unit Tests

**New Test Files** (~500 lines total):

- `/src/lib/agents/hypothesis-agent.test.ts` - Agent methods
- `/src/lib/ab-testing.test.ts` - Extended statistical functions
- `/src/lib/workflows/actions/hypothesis-agent-actions.test.ts` - Step handlers
- `/src/app/api/orbit/[workspaceSlug]/experiments/**/*.test.ts` - API routes

**Coverage Requirements**:

- 100% coverage for statistical functions (confidence intervals, effect size)
- 100% coverage for API routes
- Mock AI responses for hypothesis generation
- Mock database with in-memory Prisma client

### Integration Tests

**E2E Test Scenarios** (Cucumber features):

```gherkin
# e2e/features/hypothesis-agent.feature

Feature: Hypothesis AI Agent

  Scenario: Generate hypotheses from workspace data
    Given I am logged in to workspace "test-workspace"
    And I have historical campaign data
    When I click "Generate Hypotheses"
    Then I should see 3 to 5 hypotheses
    And each hypothesis should have a confidence score
    And hypotheses should be ranked by priority

  Scenario: Approve hypothesis and create experiment
    Given I have a proposed hypothesis "Shorter headlines improve CTR"
    When I click "Approve & Test"
    And I configure 3 variants
    And I select "engagement_rate" as primary metric
    Then an A/B test should be created
    And variants should be generated using AI
    And the experiment status should be "DRAFT"

  Scenario: Analyze experiment results with auto-winner selection
    Given I have a running experiment with sufficient data
    And auto-winner selection is enabled
    When the experiment reaches statistical significance
    Then the winner should be automatically selected
    And the experiment status should be "COMPLETED"
    And I should see confidence intervals for all variants
```

**Test Implementation**:

- `/e2e/step-definitions/hypothesis-agent.steps.ts` (~200 lines)
- Use Playwright to interact with UI
- Seed database with test experiments
- Mock AI responses for reproducibility

### Performance Tests

**Metrics to Track**:

- Hypothesis generation time: < 10s for 5 hypotheses
- Variant content generation: < 5s per variant
- Statistical analysis: < 1s for 10,000 data points
- API response times: < 500ms for all endpoints

---

## Migration Plan

### Phase 1: Database Schema (Week 1)

**Tasks**:

1. ✅ Create Prisma migration for `Hypothesis` and `ExperimentResult` models
2. ✅ Seed database with sample hypotheses for testing
3. ✅ Add optional `hypothesisId` to `SocialPostAbTest`
4. ✅ Run migration on dev/staging environments

**Commands**:

```bash
# Create migration
npx prisma migrate dev --name add-hypothesis-agent-models

# Seed with sample data
npx prisma db seed
```

**Validation**:

- Run `npx prisma studio` to inspect new models
- Verify foreign key relationships work correctly
- Test cascade deletes (workspace deletion)

### Phase 2: Core Agent Implementation (Week 2-3)

**Tasks**:

1. ✅ Implement `HypothesisAgent` class
2. ✅ Extend `ab-testing.ts` with confidence intervals and effect size
3. ✅ Create workflow step handlers
4. ✅ Write unit tests (100% coverage)

**Files to Create**:

- `/src/lib/agents/hypothesis-agent.ts`
- `/src/lib/agents/hypothesis-agent.test.ts`
- `/src/lib/workflows/actions/hypothesis-agent-actions.ts`
- `/src/lib/workflows/actions/hypothesis-agent-actions.test.ts`

**Validation**:

- All unit tests pass
- Coverage report shows 100%
- Agent can generate mock hypotheses

### Phase 3: API Routes (Week 4)

**Tasks**:

1. ✅ Implement API routes for hypothesis CRUD
2. ✅ Implement experiment analysis and winner selection endpoints
3. ✅ Add authentication and authorization checks
4. ✅ Write API route tests

**Files to Create**:

- 6 API route files (see "API Routes" section)
- Corresponding test files

**Validation**:

- Postman/Insomnia collection for manual testing
- All API tests pass
- Rate limiting works correctly

### Phase 4: UI Components (Week 5-6)

**Tasks**:

1. ✅ Build hypothesis list and card components
2. ✅ Build experiment designer UI
3. ✅ Build results dashboard with charts
4. ✅ Integrate with existing Orbit navigation

**Files to Create**:

- 10+ React components (see "UI Components" section)
- 3 page routes

**Validation**:

- UI matches Figma designs (if available)
- Accessibility audit passes (WCAG 2.1 AA)
- Mobile responsive

### Phase 5: Integration & E2E Testing (Week 7)

**Tasks**:

1. ✅ Write Cucumber feature files
2. ✅ Implement Playwright step definitions
3. ✅ Test full hypothesis → experiment → results flow
4. ✅ Fix any integration bugs

**Validation**:

- All E2E tests pass
- Manual QA checklist completed
- Performance metrics meet targets

### Phase 6: Documentation & Launch (Week 8)

**Tasks**:

1. ✅ Update user documentation in `/docs/FEATURES.md`
2. ✅ Create video tutorial (optional)
3. ✅ Deploy to staging for beta testing
4. ✅ Deploy to production

**Validation**:

- Beta users can complete full workflow
- No critical bugs in production
- Analytics tracking works correctly

---

## Files to Create

### Backend (Estimated ~1,500 lines)

| File                                                                              | Lines | Purpose                   |
| --------------------------------------------------------------------------------- | ----- | ------------------------- |
| `/src/lib/agents/hypothesis-agent.ts`                                             | 400   | Core agent implementation |
| `/src/lib/agents/hypothesis-agent.test.ts`                                        | 300   | Unit tests for agent      |
| `/src/lib/workflows/actions/hypothesis-agent-actions.ts`                          | 200   | Workflow step handlers    |
| `/src/lib/workflows/actions/hypothesis-agent-actions.test.ts`                     | 150   | Step handler tests        |
| `/src/app/api/orbit/[workspaceSlug]/experiments/hypotheses/generate/route.ts`     | 100   | Generate API              |
| `/src/app/api/orbit/[workspaceSlug]/experiments/hypotheses/route.ts`              | 80    | List API                  |
| `/src/app/api/orbit/[workspaceSlug]/experiments/hypotheses/[id]/approve/route.ts` | 120   | Approve API               |
| `/src/app/api/orbit/[workspaceSlug]/experiments/[id]/analyze/route.ts`            | 150   | Analysis API              |
| `/src/app/api/orbit/[workspaceSlug]/experiments/[id]/select-winner/route.ts`      | 100   | Winner API                |
| `/src/app/api/orbit/[workspaceSlug]/experiments/[id]/results/route.ts`            | 90    | Results API               |

### Frontend (Estimated ~1,200 lines)

| File                                                              | Lines | Purpose           |
| ----------------------------------------------------------------- | ----- | ----------------- |
| `/src/components/orbit/experiments/hypothesis-list.tsx`           | 120   | List component    |
| `/src/components/orbit/experiments/hypothesis-card.tsx`           | 100   | Card component    |
| `/src/components/orbit/experiments/hypothesis-generator.tsx`      | 80    | Generator UI      |
| `/src/components/orbit/experiments/experiment-designer.tsx`       | 150   | Designer UI       |
| `/src/components/orbit/experiments/variant-preview.tsx`           | 100   | Preview component |
| `/src/components/orbit/experiments/results-dashboard.tsx`         | 200   | Dashboard         |
| `/src/components/orbit/experiments/confidence-interval-chart.tsx` | 120   | Chart component   |
| `/src/components/orbit/experiments/significance-badge.tsx`        | 50    | Badge component   |
| `/src/components/orbit/experiments/winner-announcement.tsx`       | 80    | Winner UI         |
| `/src/components/orbit/experiments/auto-winner-toggle.tsx`        | 50    | Toggle component  |
| `/src/app/orbit/[workspaceSlug]/experiments/page.tsx`             | 250   | Dashboard page    |
| `/src/app/orbit/[workspaceSlug]/experiments/hypotheses/page.tsx`  | 200   | Hypotheses page   |
| `/src/app/orbit/[workspaceSlug]/experiments/[id]/page.tsx`        | 300   | Detail page       |

### Testing (Estimated ~700 lines)

| File                                              | Lines | Purpose            |
| ------------------------------------------------- | ----- | ------------------ |
| `/e2e/features/hypothesis-agent.feature`          | 100   | Cucumber scenarios |
| `/e2e/step-definitions/hypothesis-agent.steps.ts` | 200   | Playwright steps   |
| API route test files (6 files × 50 lines)         | 300   | API tests          |
| Component test files (10 files × 30 lines)        | 300   | Component tests    |

### Database

| File                                                                      | Lines | Purpose          |
| ------------------------------------------------------------------------- | ----- | ---------------- |
| `prisma/migrations/{timestamp}_add_hypothesis_agent_models/migration.sql` | 100   | Schema migration |
| `prisma/seed.ts` (additions)                                              | 50    | Seed data        |

**Total Estimated Lines of Code**: ~3,550 lines

---

## Files to Modify

### Schema Extensions

| File                   | Changes                                                          | Lines Added |
| ---------------------- | ---------------------------------------------------------------- | ----------- |
| `prisma/schema.prisma` | Add Hypothesis, ExperimentResult models; extend SocialPostAbTest | ~120        |

### Statistical Library

| File                     | Changes                                        | Lines Added |
| ------------------------ | ---------------------------------------------- | ----------- |
| `/src/lib/ab-testing.ts` | Add confidence interval, effect size functions | ~50         |

### Existing UI Components

| File                                               | Changes                           | Lines Added |
| -------------------------------------------------- | --------------------------------- | ----------- |
| `/src/components/orbit/ab-tests/ab-test-card.tsx`  | Show linked hypothesis if present | ~15         |
| `/src/app/orbit/[workspaceSlug]/ab-tests/page.tsx` | Add "Generate Hypotheses" CTA     | ~10         |

### Publishing System (Optional - for Phase 2)

| File                                                 | Changes                          | Lines Added |
| ---------------------------------------------------- | -------------------------------- | ----------- |
| `/src/lib/calendar/publishing-service.ts`            | Support variant content delivery | ~30         |
| `/src/app/api/cron/publish-scheduled-posts/route.ts` | Track experiment impressions     | ~20         |

### Analytics (Optional - for Phase 2)

| File                                      | Changes                      | Lines Added |
| ----------------------------------------- | ---------------------------- | ----------- |
| `/src/lib/analytics/analytics-service.ts` | Aggregate experiment results | ~40         |
| `/src/app/api/tracking/event/route.ts`    | Tag events with variant ID   | ~10         |

**Total Lines Modified**: ~295 lines across 8 files

---

## Potential Risks & Blockers

### Risk 1: AI Hypothesis Quality

**Description**: AI-generated hypotheses may be low quality or irrelevant
**Mitigation**:

- Implement confidence scoring (0-1)
- Require user approval before testing
- Use few-shot prompting with high-quality examples
- Incorporate historical data for context

**Fallback**: Manual hypothesis creation workflow

### Risk 2: Statistical Complexity

**Description**: Chi-squared tests may not be appropriate for all metrics
**Mitigation**:

- Support multiple test types (chi-squared, t-test, Mann-Whitney U)
- Validate assumptions before applying test
- Provide clear warnings when sample size is insufficient

**Fallback**: Display raw metrics without statistical inference

### Risk 3: Integration with Publishing System

**Description**: Variant delivery may conflict with existing scheduling logic
**Mitigation**:

- Use metadata field for variant-specific content
- Implement feature flag for gradual rollout
- Extensive testing with staging social accounts

**Fallback**: Manual variant publishing workflow

### Risk 4: Performance (Large Experiments)

**Description**: Analyzing experiments with >100k data points may be slow
**Mitigation**:

- Use database aggregation instead of in-memory calculation
- Implement caching for frequently accessed results
- Add loading states and async analysis

**Fallback**: Pre-compute results on cron job

### Risk 5: Subscription Limits

**Description**: Free tier users may abuse hypothesis generation
**Mitigation**:

- Enforce rate limits (e.g., 10 hypotheses/day for free tier)
- Add subscription checks before generation
- Use quota tracking in `SubscriptionUsage` model

**Fallback**: Manual hypothesis creation only

---

## Out of Scope (For This Epic)

To maintain focus and avoid scope creep, the following are explicitly **NOT** included in this epic:

### 1. Multi-Variate Testing (MVT)

- Testing multiple variables simultaneously (e.g., headline + image + CTA)
- **Reason**: Adds significant complexity to statistical analysis
- **Future Epic**: ORB-XXX (Multi-Variate Testing)

### 2. Sequential Testing (Bayesian A/B Testing)

- Continuous monitoring with early stopping rules
- **Reason**: Requires different statistical framework
- **Future Epic**: ORB-XXX (Bayesian Experimentation)

### 3. Personalization / Contextual Bandits

- Adapting variants based on user characteristics
- **Reason**: Requires user segmentation and ML infrastructure
- **Future Epic**: ORB-XXX (Personalization Engine)

### 4. Cross-Platform Experiments

- Testing same hypothesis across multiple social platforms
- **Reason**: Requires platform-specific variant adaptation
- **Future Enhancement**: Can be added to Phase 2

### 5. Automated Experiment Discovery

- AI automatically identifying opportunities to experiment (without user trigger)
- **Reason**: Requires anomaly detection and proactive alerting system
- **Future Epic**: ORB-XXX (Autonomous Optimization)

### 6. A/B Testing for Non-Content

- Testing pricing, UI changes, email campaigns
- **Reason**: Different delivery and tracking mechanisms
- **Future Epics**: Separate epics for each domain

---

## Success Metrics

### North Star Metric

**Hypothesis → Winner Cycle Time**: Average time from hypothesis generation to winner selection

**Target**: < 7 days for content experiments

### Key Performance Indicators (KPIs)

| Metric                                        | Target               | Measurement                     |
| --------------------------------------------- | -------------------- | ------------------------------- |
| Hypothesis Generation Time                    | < 10s                | AI agent latency                |
| Hypothesis Approval Rate                      | > 60%                | Approved / Total Generated      |
| Experiment Completion Rate                    | > 80%                | Completed / Started             |
| Statistical Significance Achievement          | > 70%                | Significant / Total Experiments |
| Winner Lift (Avg)                             | > 10%                | Winner Metric / Control Metric  |
| User Engagement (Weekly Active Experimenters) | > 20% of Orbit users | Unique users using feature      |

### Monitoring & Alerts

**Dashboards** (internal analytics):

- Hypothesis generation trends (daily count)
- Experiment funnel (proposed → testing → validated)
- Winner lift distribution (histogram)
- Feature adoption rate (weekly active users)

**Alerts** (send to Slack/email):

- AI hypothesis generation failures (> 5% error rate)
- Experiments stuck in "TESTING" > 14 days
- Winner selection errors

---

## Technical Debt & Future Enhancements

### Immediate Tech Debt (Address in Phase 2)

1. **A/B Test Model Generalization**
   - Current: `SocialPostAbTest` is content-specific
   - Future: Generic `Experiment` model for any testable unit
   - Migration: Rename and abstract model

2. **Statistical Test Selection**
   - Current: Chi-squared only
   - Future: Auto-select test based on data distribution
   - Enhancement: Add normality tests, Shapiro-Wilk

3. **Variant Assignment Logic**
   - Current: Random split (assumed)
   - Future: Deterministic hash-based assignment
   - Reason: Prevent variant drift if user revisits

### Long-Term Enhancements (Future Epics)

1. **Reinforcement Learning for Variant Selection**
   - Use Thompson Sampling for multi-armed bandit optimization
   - Epic: ORB-XXX

2. **Experiment Collaboration**
   - Share hypotheses across workspaces
   - Community hypothesis library
   - Epic: ORB-XXX

3. **Meta-Analysis**
   - Aggregate learnings across experiments
   - Identify universal patterns (e.g., "shorter headlines always win")
   - Epic: ORB-XXX

---

## Dependencies

### External Dependencies

- ✅ **Claude API** (Anthropic) - For hypothesis and variant generation
- ✅ **Vercel Sandbox** - For running agent in isolated environment
- ✅ **jStat** (already in use) - Statistical calculations
- ⚠️ **Chart.js or Recharts** - For confidence interval visualization (NEW)

### Internal Dependencies

- ✅ **Workflow Orchestration System** (Epic 3.3) - SATISFIED (workflow-executor.ts exists)
- ✅ **A/B Testing Models** - SATISFIED (SocialPostAbTest, AbTestVariant, AbTestResult)
- ✅ **Publishing System** - SATISFIED (publishing-service.ts, cron job)
- ✅ **Analytics Tracking** - SATISFIED (VisitorSession, AnalyticsEvent, CampaignAttribution)
- ✅ **Brand Brain** - SATISFIED (BrandProfile, BrandGuardrail, BrandVocabulary)

**No Blockers** - All dependencies satisfied by existing infrastructure.

---

## Open Questions

### For Product Team

1. **Hypothesis Approval Workflow**: Should hypotheses auto-approve after N days of no action?
2. **Auto-Winner Promotion**: Should winning variant automatically replace original content?
3. **Pricing**: How many hypotheses/experiments allowed per subscription tier?
4. **Notification Preferences**: When should users be notified (hypothesis ready, experiment complete, winner selected)?

### For Engineering Team

1. **Chart Library**: Recharts vs Chart.js for confidence interval visualization?
2. **AI Model Selection**: Claude Sonnet vs Haiku for hypothesis generation (cost vs quality)?
3. **Caching Strategy**: Redis or in-memory for experiment result caching?
4. **Database Indexes**: Which queries will be most frequent (optimize indexes accordingly)?

### For UX/Design Team

1. **Hypothesis Display**: How to show AI confidence score without overwhelming users?
2. **Experiment Status**: Visual design for lifecycle states (PROPOSED → TESTING → VALIDATED)?
3. **Winner Announcement**: Celebration animation or simple badge?
4. **Mobile Experience**: Full feature parity or desktop-first?

---

## Appendix A: Statistical Test Reference

### Chi-Squared Test (Current Implementation)

- **Use Case**: Categorical data (e.g., clicked vs not clicked)
- **Assumptions**: Expected frequency ≥ 5 in all cells
- **Formula**: χ² = Σ((Observed - Expected)² / Expected)
- **Implementation**: `/src/lib/ab-testing.ts` lines 45-67

### Two-Proportion Z-Test (Recommended Addition)

- **Use Case**: Comparing two conversion rates
- **Assumptions**: np ≥ 5 and n(1-p) ≥ 5
- **Formula**: z = (p1 - p2) / √(p(1-p)(1/n1 + 1/n2))
- **Implementation**: TODO in Phase 2

### Student's T-Test (Future)

- **Use Case**: Continuous metrics (e.g., time on page, revenue per user)
- **Assumptions**: Normal distribution, equal variances
- **Formula**: t = (μ1 - μ2) / √(s²(1/n1 + 1/n2))
- **Implementation**: TODO in Phase 3

### Effect Size (Cohen's h)

- **Use Case**: Quantify practical significance
- **Interpretation**:
  - h < 0.2: Small effect
  - 0.2 ≤ h < 0.5: Medium effect
  - h ≥ 0.5: Large effect
- **Formula**: h = 2(arcsin√p1 - arcsin√p2)
- **Implementation**: `/src/lib/ab-testing.ts` (to be added)

---

## Appendix B: AI Prompt Templates

### Hypothesis Generation Prompt

```
You are a hypothesis generation agent for a social media marketing platform.

CONTEXT:
- Workspace: {workspaceName}
- Brand Voice: {brandProfile.toneDescriptors}
- Historical Performance:
  - Average Engagement Rate: {avgEngagement}%
  - Best Performing Content Type: {bestContentType}
  - Top 3 Topics: {topTopics}

TASK:
Generate {count} hypotheses for content experiments that could improve {focus}.

REQUIREMENTS:
- Each hypothesis must be testable via A/B test
- Provide theoretical basis (why might this work?)
- Estimate expected outcome (% improvement)
- Assign confidence score (0-1)
- Respect brand guardrails: {guardrails}

OUTPUT FORMAT (JSON):
{
  "hypotheses": [
    {
      "title": "Short, descriptive title",
      "description": "Full hypothesis statement",
      "theoreticalBasis": "Explanation of why this might work",
      "expectedOutcome": "Predicted result with magnitude",
      "confidence": 0.75,
      "reasoning": "AI's reasoning for this hypothesis"
    }
  ]
}
```

### Variant Generation Prompt

```
You are a content variant generation agent.

ORIGINAL CONTENT:
{originalContent}

HYPOTHESIS:
{hypothesis.description}

BRAND GUIDELINES:
- Tone: {brandProfile.toneDescriptors}
- Guardrails: {brandGuardrails}
- Preferred Vocabulary: {brandVocabulary}

TASK:
Generate {count} content variants that test the hypothesis while maintaining brand voice.

REQUIREMENTS:
- Variant 1 = Control (original content, unchanged)
- Variants 2-N = Test variations implementing the hypothesis
- Each variant must respect all brand guardrails
- Use preferred vocabulary where applicable
- Maintain similar length (±20%)

OUTPUT FORMAT (JSON):
{
  "variants": [
    {
      "id": "control",
      "content": "Original content",
      "variationType": "control"
    },
    {
      "id": "variant_1",
      "content": "Modified content testing hypothesis",
      "variationType": "test_variation",
      "changes": "Description of what changed"
    }
  ]
}
```

### Results Interpretation Prompt

```
You are an experiment analysis agent.

EXPERIMENT:
- Hypothesis: {hypothesis.description}
- Variants: {variantCount}
- Primary Metric: {primaryMetric}

STATISTICAL RESULTS:
{resultsData}

TASK:
Interpret the experiment results and provide actionable insights.

OUTPUT FORMAT (JSON):
{
  "isSignificant": true/false,
  "winner": "variant_id or null",
  "interpretation": "Plain English explanation of results",
  "insights": [
    "Insight 1",
    "Insight 2"
  ],
  "recommendations": [
    "Recommendation 1",
    "Recommendation 2"
  ],
  "nextSteps": "What to do next based on results"
}
```

---

## Appendix C: Database Queries (Optimization)

### Key Queries to Optimize

```sql
-- Query 1: List active experiments with hypotheses
SELECT e.*, h.*
FROM "SocialPostAbTest" e
LEFT JOIN "Hypothesis" h ON e."id" = h."experimentId"
WHERE e."workspaceId" = ? AND e."status" = 'RUNNING'
ORDER BY e."createdAt" DESC;

-- Recommended Index:
CREATE INDEX idx_experiments_workspace_status
ON "SocialPostAbTest"("workspaceId", "status", "createdAt" DESC);

-- Query 2: Aggregate experiment results by variant
SELECT
  v."id",
  v."name",
  COUNT(r."id") AS impressions,
  SUM(r."converted"::int) AS conversions,
  AVG(r."converted"::int) AS conversion_rate
FROM "SocialPostAbTestVariant" v
LEFT JOIN "AbTestResult" r ON v."id" = r."abTestVariantId"
WHERE v."abTestId" = ?
GROUP BY v."id", v."name";

-- Recommended Index:
CREATE INDEX idx_results_variant
ON "AbTestResult"("abTestVariantId", "converted");

-- Query 3: Fetch hypotheses by status and priority
SELECT *
FROM "Hypothesis"
WHERE "workspaceId" = ? AND "status" = 'PROPOSED'
ORDER BY "priority" DESC, "confidence" DESC
LIMIT 10;

-- Recommended Index (already planned in schema):
-- @@index([workspaceId])
-- @@index([status])
-- @@index([priority])
```

---

## Conclusion

This implementation plan provides a comprehensive roadmap for building the Hypothesis AI Agent - Experimentation Framework. The plan leverages existing infrastructure (A/B testing, workflows, publishing, analytics) while adding AI-powered hypothesis generation and automated experiment orchestration.

**Key Highlights**:

- ✅ All dependencies satisfied (no blockers)
- ✅ Clear phased rollout (8 weeks)
- ✅ Detailed file-by-file implementation guide
- ✅ 100% test coverage requirement
- ✅ Integration with existing systems
- ✅ Statistical rigor (confidence intervals, effect size, significance testing)
- ✅ Extensible architecture for future enhancements

**Next Steps**:

1. Review and approve this plan
2. Create GitHub issues for ORB-046 and ORB-047 with acceptance criteria
3. Begin Phase 1: Database schema migration
4. Kick off development in weekly sprints

---

**Plan Author**: Claude (Hypothesis Planning Agent)
**Date**: 2026-01-29
**Status**: Ready for Review
